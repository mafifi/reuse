[
	{
		"id": "business",
		"name": "Business Architecture",
		"description": "Business capabilities, processes, and organizational structures",
		"icon": "Building2",
		"color": "bg-midnight-blue-50 border-midnight-blue-200 hover:bg-midnight-blue-100",
		"categories": [
			{
				"id": "institutional-securities-sales-trading",
				"name": "Institutional Securities - Sales & Trading",
				"description": "Capabilities supporting institutional securities, sales, and trading operations.",
				"subcategories": [
					{
						"id": "cash-equities",
						"name": "Cash Equities",
						"description": "Capabilities for cash equities trading and related services.",
						"capabilities": [
							{
								"id": "client-management-services",
								"name": "Client Management Services",
								"definition": "Services and processes for managing institutional client relationships, onboarding, and support.",
								"description": "Client Management Services are essential for maintaining strong relationships with institutional clients. These services include CRM systems, onboarding processes, customer service, and feedback management.",
								"examples": [
									"Customer Relationship Management (CRM) systems",
									"Customer onboarding processes",
									"Loyalty and rewards programs",
									"Customer service and support",
									"Customer segmentation and targeting",
									"Customer feedback management"
								],
								"benefits": [
									"Improved customer satisfaction and loyalty",
									"Increased customer lifetime value",
									"Better customer insights and analytics",
									"Enhanced customer experience",
									"Reduced customer acquisition costs",
									"Improved customer retention rates"
								],
								"keyComponents": [
									"Customer data management",
									"Interaction tracking",
									"Service delivery",
									"Feedback collection",
									"Relationship analytics"
								],
								"technologies": [
									"Salesforce CRM",
									"Microsoft Dynamics 365",
									"HubSpot",
									"Oracle CX Cloud",
									"SAP Customer Experience"
								],
								"maturityLevels": {
									"plan": { 
										"available": true, 
										"description": "Client relationship frameworks, institutional CRM selection guides, and customer engagement patterns available" 
									},
									"build": { 
										"available": true, 
										"description": "Institutional CRM platforms, client onboarding systems, and relationship management tools available" 
									},
									"operate": { 
										"available": true, 
										"description": "Managed client relationship services, CRM-as-a-service platforms, and client analytics solutions available" 
									},
									"score": 3
								}
							},
							{
								"id": "sales-distribution",
								"name": "Sales & Distribution",
								"definition": "Activities related to the distribution and sale of cash equity products to institutional clients."
							},
							{
								"id": "modelling-research-activities",
								"name": "Modelling & Research Activities",
								"definition": "Research, analytics, and quantitative modeling to support trading and investment decisions."
							},
							{
								"id": "data-analytics",
								"name": "Data & Analytics",
								"definition": "Data management, analytics, and reporting for trading, client, and market insights."
							},
							{
								"id": "front-office-risk",
								"name": "Front Office Risk",
								"definition": "Risk management activities performed by the front office, including real-time risk assessment and controls."
							},
							{
								"id": "transaction-processing-services",
								"name": "Transaction Processing Services",
								"definition": "Processing, clearing, and settlement of cash equity transactions."
							},
							{
								"id": "firm-risk-cross-divisional-governance",
								"name": "Firm Risk & Cross Divisional Governance",
								"definition": "Oversight, governance, and risk management across divisions and at the firm level."
							},
							{
								"id": "operations",
								"name": "Operations",
								"definition": "Operational support for trade lifecycle, reconciliations, and exception management."
							},
							{
								"id": "finance",
								"name": "Finance",
								"definition": "Financial management, reporting, and control for cash equities trading."
							}
						]
					}
				]
			}
		]
	},
	{
		"id": "application",
		"name": "Application Architecture",
		"description": "Software applications and their interactions within the enterprise",
		"icon": "Layers",
		"color": "bg-watercourse-50 border-watercourse-200 hover:bg-watercourse-100",
		"categories": [
			{
				"id": "application-engineering",
				"name": "Application Engineering",
				"description": "Core business applications and engineering tools for application functionality and logic",
				"subcategories": [
					{
						"id": "enterprise-applications",
						"name": "Enterprise Applications",
						"description": "Core business applications and systems",
						"capabilities": [
							{
								"id": "enterprise-resource-planning",
								"name": "Enterprise Resource Planning",
								"definition": "Integrated software platform managing core business processes",
								"description": "Enterprise Resource Planning (ERP) provides a unified suite of applications that streamline and automate critical business processes such as finance, procurement, supply chain, and human resources. ERP enables consistent data sharing, process efficiency, and real-time insights across the enterprise.",
								"examples": [
									"Finance and accounting management",
									"Procurement and supplier management",
									"Inventory and warehouse management",
									"Order processing and fulfilment",
									"Human resources management",
									"Project and resource planning"
								],
								"benefits": [
									"End-to-end process integration",
									"Improved operational efficiency",
									"Real-time business insights",
									"Data consistency across departments",
									"Scalability for growth",
									"Regulatory compliance support"
								],
								"keyComponents": [
									"Core business process modules",
									"Data integration layer",
									"Reporting and analytics tools",
									"Security and access controls",
									"Workflow automation"
								],
								"technologies": [
									"SAP S/4HANA",
									"Oracle ERP Cloud",
									"Microsoft Dynamics 365 Finance & Operations",
									"Infor CloudSuite",
									"Workday",
									"Netsuite ERP"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "ERP selection frameworks, business process modeling templates, and implementation roadmaps available"
									},
									"build": {
										"available": true,
										"description": "ERP platforms (SAP S/4HANA, Oracle ERP Cloud), integration tools, and customization frameworks available"
									},
									"operate": {
										"available": true,
										"description": "Managed ERP services, cloud ERP platforms, and enterprise support services available"
									},
									"score": 3
								}
							},
							{
								"id": "customer-relationship-management",
								"name": "Customer Relationship Management",
								"definition": "The capability to manage customer interactions and relationships through integrated technology platforms",
								"description": "Customer Relationship Management (CRM) provides centralized platforms for tracking customer interactions, managing sales pipelines, and coordinating marketing efforts. CRM systems enable organizations to maintain consistent customer experiences across touchpoints while providing analytics and automation capabilities.",
								"examples": [
									"Sales pipeline management",
									"Customer interaction tracking",
									"Marketing campaign automation",
									"Service case management",
									"Customer segmentation and targeting",
									"Lead scoring and qualification"
								],
								"benefits": [
									"Improved customer relationship quality",
									"Increased sales conversion rates",
									"Enhanced customer service efficiency",
									"Better cross-team coordination",
									"Data-driven customer insights",
									"Automated sales and marketing processes"
								],
								"keyComponents": [
									"Contact and account management",
									"Sales pipeline tracking",
									"Marketing automation",
									"Customer service integration",
									"Analytics and reporting"
								],
								"technologies": [
									"Salesforce CRM",
									"HubSpot",
									"Microsoft Dynamics 365",
									"Pipedrive",
									"Zoho CRM"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "CRM strategy frameworks, sales process design templates, and customer journey mapping guides available"
									},
									"build": {
										"available": true,
										"description": "CRM platforms (Salesforce, HubSpot), integration APIs, and customization tools available"
									},
									"operate": {
										"available": true,
										"description": "Managed CRM services, sales automation platforms, and customer analytics solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "business-intelligence",
								"name": "Business Intelligence",
								"definition": "Analytics and reporting capabilities for data-driven decision making",
								"description": "Business Intelligence (BI) enables organizations to transform raw data into actionable insights through reporting, dashboards, and interactive analytics. BI helps stakeholders monitor performance, identify trends, and make informed strategic decisions.",
								"examples": [
									"Executive dashboards",
									"Self-service reporting",
									"Ad-hoc data analysis",
									"Performance scorecards",
									"Predictive analytics",
									"Data visualization"
								],
								"benefits": [
									"Improved decision making",
									"Data-driven culture",
									"Faster insight generation",
									"Enhanced operational visibility",
									"Early risk identification",
									"Competitive advantage"
								],
								"keyComponents": [
									"Data extraction and transformation",
									"Data warehouse or data marts",
									"Visualization tools",
									"Self-service analytics",
									"Governance and security controls"
								],
								"technologies": [
									"Tableau",
									"Microsoft Power BI",
									"Qlik Sense",
									"Looker",
									"SAP BusinessObjects",
									"Domo"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "BI strategy frameworks, dashboard design templates, and analytics methodology guides available"
									},
									"build": {
										"available": true,
										"description": "BI platforms (Tableau, Power BI), data connectors, and visualization libraries available"
									},
									"operate": {
										"available": true,
										"description": "Managed BI services, cloud analytics platforms, and data visualization solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "api-management",
								"name": "API Management",
								"definition": "The capability to design, deploy, secure, and manage APIs across the enterprise",
								"description": "API Management provides comprehensive lifecycle management for Application Programming Interfaces (APIs), including design, security, monitoring, and governance. It enables organizations to expose internal services, integrate with partners, and build API-driven architectures while maintaining security and performance.",
								"examples": [
									"API gateway implementation",
									"API security and authentication",
									"API documentation and developer portals",
									"API versioning and lifecycle management",
									"API analytics and monitoring",
									"Rate limiting and throttling"
								],
								"benefits": [
									"Accelerated integration and development",
									"Enhanced API security and governance",
									"Improved developer experience",
									"Better API performance monitoring",
									"Standardized API design patterns",
									"Reduced integration complexity"
								],
								"keyComponents": [
									"API gateway",
									"Developer portal",
									"API security layer",
									"Analytics and monitoring",
									"API lifecycle management"
								],
								"technologies": [
									"Kong",
									"Apigee",
									"Azure API Management",
									"AWS API Gateway",
									"MuleSoft Anypoint"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "API design patterns, governance frameworks, and REST/GraphQL best practices available"
									},
									"build": {
										"available": true,
										"description": "API gateways (Kong, Apigee), development frameworks, and testing tools available"
									},
									"operate": {
										"available": true,
										"description": "Managed API services, cloud API platforms, and enterprise API management solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "microservices-architecture",
								"name": "Microservices Architecture",
								"definition": "The capability to design, build, and operate applications as a collection of loosely coupled, independently deployable services",
								"description": "Microservices Architecture enables organizations to decompose complex applications into smaller, manageable services that can be developed, deployed, and scaled independently. It supports rapid development cycles, technology diversity, and organizational scaling while improving system resilience and maintainability.",
								"examples": [
									"Service decomposition and domain modeling",
									"Inter-service communication patterns",
									"Independent service deployment",
									"Distributed data management",
									"Service discovery and registration",
									"Circuit breaker and resilience patterns"
								],
								"benefits": [
									"Improved development team autonomy",
									"Enhanced system scalability",
									"Faster deployment cycles",
									"Better fault isolation",
									"Technology diversity and flexibility",
									"Improved system maintainability"
								],
								"keyComponents": [
									"Service boundaries and APIs",
									"Service discovery mechanisms",
									"Inter-service communication",
									"Distributed data management",
									"Service monitoring and observability"
								],
								"technologies": ["Spring Boot", "Netflix OSS", "Istio", "Consul", "Eureka"],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Microservices patterns, domain-driven design frameworks, and service decomposition guides available"
									},
									"build": {
										"available": true,
										"description": "Container platforms (Docker, Kubernetes), service mesh tools, and microservices frameworks available"
									},
									"operate": {
										"available": true,
										"description": "Managed container services, cloud-native platforms, and service mesh solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "container-orchestration",
								"name": "Container Orchestration",
								"definition": "The capability to automate the deployment, scaling, and management of containerized applications",
								"description": "Container Orchestration provides automated management of containerized applications across clusters of hosts. It handles deployment, scaling, networking, and health monitoring of containers, enabling organizations to run applications reliably at scale while simplifying operations and resource management.",
								"examples": [
									"Automated container deployment",
									"Service discovery and load balancing",
									"Auto-scaling based on demand",
									"Rolling updates and rollbacks",
									"Health monitoring and self-healing",
									"Resource allocation and optimization"
								],
								"benefits": [
									"Simplified container management",
									"Improved application scalability",
									"Enhanced system reliability",
									"Reduced operational overhead",
									"Better resource utilization",
									"Faster deployment cycles"
								],
								"keyComponents": [
									"Container runtime",
									"Cluster management",
									"Service discovery",
									"Load balancing",
									"Health monitoring"
								],
								"technologies": ["Kubernetes", "Docker Swarm", "OpenShift", "Nomad", "Amazon ECS"],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Container orchestration patterns, Kubernetes architecture guides, and deployment strategies available"
									},
									"build": {
										"available": true,
										"description": "Container platforms (Docker, Kubernetes), orchestration tools, and CI/CD pipelines available"
									},
									"operate": {
										"available": true,
										"description": "Managed Kubernetes services (EKS, GKE, AKS), container-as-a-service platforms available"
									},
									"score": 3
								}
							},
							{
								"id": "mobile-development-frameworks",
								"name": "Mobile Development Frameworks",
								"definition": "The capability to develop and deploy mobile applications across multiple platforms using unified frameworks",
								"description": "Mobile Development Frameworks enable organizations to create mobile applications for iOS, Android, and other platforms using shared codebases and development practices. They provide cross-platform compatibility, native performance, and streamlined development workflows while reducing time-to-market and maintenance overhead.",
								"examples": [
									"Cross-platform mobile app development",
									"Native mobile app development",
									"Progressive web app (PWA) development",
									"Mobile app testing and deployment",
									"App store publishing and distribution",
									"Mobile app performance optimization"
								],
								"benefits": [
									"Reduced development time and costs",
									"Consistent user experience across platforms",
									"Shared codebase maintenance",
									"Faster time-to-market",
									"Improved developer productivity",
									"Enhanced mobile app quality"
								],
								"keyComponents": [
									"Cross-platform development tools",
									"Native platform integrations",
									"UI/UX component libraries",
									"Testing and debugging tools",
									"App deployment pipelines"
								],
								"technologies": ["React Native", "Flutter", "Xamarin", "Ionic", "Swift/Kotlin"],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Mobile app architecture patterns, cross-platform development guides, and UI/UX frameworks available"
									},
									"build": {
										"available": true,
										"description": "Mobile frameworks (React Native, Flutter), development tools, and testing platforms available"
									},
									"operate": {
										"available": true,
										"description": "App store deployment services, mobile backend platforms, and device testing clouds available"
									},
									"score": 3
								}
							},
							{
								"id": "web-frontend-frameworks",
								"name": "Web Frontend Frameworks",
								"definition": "The capability to build modern, interactive web user interfaces using component-based frameworks",
								"description": "Web Frontend Frameworks provide structured approaches to building dynamic, responsive web applications with reusable components, state management, and modern development tools. They enable rapid development of sophisticated user interfaces while maintaining code quality and performance.",
								"examples": [
									"Component-based UI development",
									"Single-page application (SPA) development",
									"Progressive web app (PWA) implementation",
									"Server-side rendering (SSR)",
									"State management and data flow",
									"Responsive design and mobile optimization"
								],
								"benefits": [
									"Faster frontend development",
									"Improved code reusability",
									"Enhanced user experience",
									"Better maintainability",
									"Modern development practices",
									"Optimized performance"
								],
								"keyComponents": [
									"Component libraries",
									"State management systems",
									"Build and bundling tools",
									"Development servers",
									"Testing frameworks"
								],
								"technologies": ["React", "Vue.js", "Angular", "Svelte", "Next.js"],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Frontend architecture patterns, component design systems, and modern web development guides available"
									},
									"build": {
										"available": true,
										"description": "Frontend frameworks (React, Vue, Angular), build tools, and component libraries available"
									},
									"operate": {
										"available": true,
										"description": "Frontend hosting platforms, CDN services, and web performance optimization tools available"
									},
									"score": 3
								}
							},
							{
								"id": "authentication-implementation",
								"name": "Authentication Implementation",
								"definition": "The capability to implement secure authentication and authorization flows within applications",
								"description": "Authentication Implementation provides developers with the tools and patterns to integrate secure login, authentication, and authorization mechanisms into applications. It covers OAuth flows, JWT handling, session management, and SSO integration while maintaining security best practices and user experience.",
								"examples": [
									"OAuth 2.0 and OpenID Connect flows",
									"JWT token management and validation",
									"Session-based authentication",
									"Multi-factor authentication (MFA) integration",
									"Single sign-on (SSO) implementation",
									"Social login integration"
								],
								"benefits": [
									"Secure user authentication",
									"Simplified login experience",
									"Reduced authentication complexity",
									"Standardized security practices",
									"Better user experience",
									"Reduced development time"
								],
								"keyComponents": [
									"Authentication libraries",
									"Token management",
									"Session handling",
									"Authorization middleware",
									"Security validation"
								],
								"technologies": [
									"Auth0",
									"Firebase Auth",
									"NextAuth.js",
									"Passport.js",
									"Keycloak"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Authentication patterns, security frameworks, and identity management guides available"
									},
									"build": {
										"available": true,
										"description": "Auth libraries (Auth0, Firebase Auth), identity providers, and security frameworks available"
									},
									"operate": {
										"available": true,
										"description": "Managed identity services, SSO platforms, and authentication-as-a-service solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "application-performance-monitoring",
								"name": "Application Performance Monitoring",
								"definition": "The capability to monitor, analyze, and optimize application performance and user experience",
								"description": "Application Performance Monitoring (APM) provides real-time visibility into application behavior, performance metrics, and user experience. It enables developers and operations teams to identify bottlenecks, troubleshoot issues, and optimize application performance across the entire technology stack.",
								"examples": [
									"Real-time performance monitoring",
									"Application dependency mapping",
									"Error tracking and debugging",
									"User experience monitoring",
									"Database query performance analysis",
									"Code-level performance profiling"
								],
								"benefits": [
									"Faster issue identification and resolution",
									"Improved application performance",
									"Enhanced user experience",
									"Proactive performance optimization",
									"Reduced mean time to resolution",
									"Better resource utilization"
								],
								"keyComponents": [
									"Performance monitoring agents",
									"Real-time analytics engine",
									"Alerting and notification systems",
									"Performance dashboards",
									"Root cause analysis tools"
								],
								"technologies": ["New Relic", "Datadog", "AppDynamics", "Dynatrace", "Elastic APM"],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "APM strategy frameworks, monitoring patterns, and observability guides available"
									},
									"build": {
										"available": true,
										"description": "APM tools (New Relic, Datadog), monitoring libraries, and instrumentation frameworks available"
									},
									"operate": {
										"available": true,
										"description": "Managed APM services, cloud monitoring platforms, and performance analytics solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "testing-frameworks",
								"name": "Testing Frameworks",
								"definition": "The capability to implement automated testing strategies for applications across unit, integration, and end-to-end levels",
								"description": "Testing Frameworks provide structured approaches to validate application functionality, performance, and quality through automated testing. They enable continuous validation of code changes, regression detection, and confidence in application releases while reducing manual testing effort.",
								"examples": [
									"Unit testing implementation",
									"Integration testing automation",
									"End-to-end testing scenarios",
									"Performance and load testing",
									"Test-driven development (TDD)",
									"Behavior-driven development (BDD)"
								],
								"benefits": [
									"Improved software quality",
									"Faster defect detection",
									"Reduced manual testing effort",
									"Increased deployment confidence",
									"Better code maintainability",
									"Faster development cycles"
								],
								"keyComponents": [
									"Test execution engines",
									"Test data management",
									"Assertion libraries",
									"Test reporting tools",
									"Continuous integration integration"
								],
								"technologies": ["Jest", "Cypress", "Playwright", "JUnit", "Selenium"],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Testing strategies, QA frameworks, and test automation patterns available"
									},
									"build": {
										"available": true,
										"description": "Testing frameworks (Jest, Cypress), automation tools, and CI/CD integration available"
									},
									"operate": {
										"available": true,
										"description": "Managed testing services, cloud testing platforms, and QA automation solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "real-time-communication",
								"name": "Real-time Communication",
								"definition": "The capability to enable bidirectional, real-time communication between clients and servers in web applications",
								"description": "Real-time Communication provides the infrastructure and protocols for instant data exchange between clients and servers, enabling features like live chat, real-time notifications, collaborative editing, and live updates. It supports modern user expectations for responsive, interactive web applications.",
								"examples": [
									"WebSocket connections for live data",
									"Server-sent events for updates",
									"Real-time chat and messaging",
									"Live collaborative editing",
									"Real-time notifications",
									"Live data visualization and dashboards"
								],
								"benefits": [
									"Enhanced user engagement",
									"Improved application responsiveness",
									"Better collaborative features",
									"Reduced server polling overhead",
									"Modern user experience",
									"Efficient data synchronization"
								],
								"keyComponents": [
									"WebSocket servers",
									"Message brokers",
									"Client-side real-time libraries",
									"Connection management",
									"Message routing and delivery"
								],
								"technologies": [
									"Socket.io",
									"WebSockets",
									"Server-Sent Events",
									"WebRTC",
									"Pusher"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Real-time architecture patterns, WebSocket design guides, and event-driven system blueprints available"
									},
									"build": {
										"available": true,
										"description": "Real-time frameworks (Socket.io, WebRTC), messaging systems, and streaming platforms available"
									},
									"operate": {
										"available": true,
										"description": "Managed real-time services, cloud messaging platforms, and streaming analytics solutions available"
									},
									"score": 3
								}
							}
						]
					},
					{
						"id": "collaboration-content",
						"name": "Collaboration & Content",
						"description": "Tools for team collaboration and content management",
						"capabilities": [
							{
								"id": "collaboration-tools",
								"name": "Collaboration Tools",
								"definition": "Platforms enabling team communication and collaboration",
								"description": "Collaboration Tools facilitate effective communication, information sharing, and teamwork within and across organizations. These tools enhance productivity, especially in distributed or hybrid work environments.",
								"examples": [
									"Team messaging platforms",
									"Video conferencing",
									"Document collaboration",
									"Project management boards",
									"Knowledge sharing portals",
									"Virtual whiteboards"
								],
								"benefits": [
									"Improved team productivity",
									"Enhanced communication",
									"Seamless remote collaboration",
									"Knowledge retention",
									"Faster decision making",
									"Cross-functional alignment"
								],
								"keyComponents": [
									"Messaging and chat",
									"File sharing",
									"Video and voice conferencing",
									"Project tracking",
									"Integration with business tools"
								],
								"technologies": [
									"Microsoft Teams",
									"Slack",
									"Zoom",
									"Google Workspace",
									"SharePoint",
									"Asana"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Collaboration strategy frameworks, digital workplace design patterns, and team productivity guides available"
									},
									"build": {
										"available": true,
										"description": "Collaboration platforms (Slack, Teams), workflow tools, and integration APIs available"
									},
									"operate": {
										"available": true,
										"description": "Managed collaboration services, enterprise communication platforms, and digital workplace solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "content-management",
								"name": "Content Management",
								"definition": "Systems for creating, managing, and publishing digital content",
								"description": "Content Management enables organizations to efficiently create, organize, store, and distribute digital content across channels. It ensures content consistency, brand integrity, and supports marketing and operational needs.",
								"examples": [
									"Website content management",
									"Document repositories",
									"Digital asset management",
									"Knowledge bases",
									"Intranet portals",
									"Multilingual content management"
								],
								"benefits": [
									"Streamlined content creation",
									"Consistent branding",
									"Improved content discoverability",
									"Faster content updates",
									"Enhanced collaboration",
									"Regulatory compliance"
								],
								"keyComponents": [
									"Content repository",
									"Authoring and editing tools",
									"Publishing workflows",
									"Search and retrieval",
									"Access control"
								],
								"technologies": [
									"SharePoint",
									"Drupal",
									"WordPress",
									"Sitecore",
									"Adobe Experience Manager",
									"Confluence"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Content strategy frameworks, information architecture guides, and content lifecycle management patterns available"
									},
									"build": {
										"available": true,
										"description": "CMS platforms (WordPress, Drupal), headless CMS solutions, and content APIs available"
									},
									"operate": {
										"available": true,
										"description": "Managed CMS services, content delivery networks, and digital asset management solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "integration-platform",
								"name": "Integration Platform",
								"definition": "Middleware connecting disparate systems and applications",
								"description": "An Integration Platform enables seamless data and process flow between different applications, systems, and services. It reduces complexity, improves agility, and supports both real-time and batch integrations across the enterprise.",
								"examples": [
									"API-based integrations",
									"Data synchronization between systems",
									"Event-driven architectures",
									"Legacy system integration",
									"Third-party service connectors",
									"B2B partner integrations"
								],
								"benefits": [
									"System interoperability",
									"Faster time-to-market",
									"Data consistency",
									"Reduced integration complexity",
									"Improved scalability",
									"Enhanced business agility"
								],
								"keyComponents": [
									"API Gateway",
									"Integration workflows",
									"Data mapping and transformation",
									"Monitoring and error handling",
									"Security and access control"
								],
								"technologies": [
									"MuleSoft Anypoint",
									"Dell Boomi",
									"Azure Logic Apps",
									"Apache Camel",
									"SnapLogic",
									"WSO2"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Integration architecture patterns, ESB design guides, and API-led connectivity frameworks available"
									},
									"build": {
										"available": true,
										"description": "Integration platforms (MuleSoft, Dell Boomi), middleware solutions, and connector libraries available"
									},
									"operate": {
										"available": true,
										"description": "Managed integration services, cloud iPaaS platforms, and enterprise service bus solutions available"
									},
									"score": 3
								}
							}
						]
					},
					{
						"id": "orchestration-logic",
						"name": "Orchestration & Logic",
						"description": "Tools for modeling, executing, and monitoring business processes",
						"capabilities": [
							{
								"id": "workflow-orchestration",
								"name": "Workflow Orchestration",
								"definition": "The ability to model, execute, and monitor multi-step business processes in an automated and maintainable way.",
								"description": "Workflow orchestration tools allow developers and operations teams to encode business logic as a sequence of tasks or state machines. These tools support retry logic, timeouts, parallelism, and integration across APIs or microservices.",
								"examples": ["Camunda", "Temporal", "Apache Airflow", "AWS Step Functions"],
								"benefits": [
									"Improved maintainability",
									"Separation of business logic",
									"Observable state transitions",
									"Simplified long-running processes"
								]
							},
							{
								"id": "business-rules-engine",
								"name": "Business Rules Engine",
								"definition": "The ability to manage and execute rule-based logic separately from application code.",
								"description": "Business rules engines allow non-developers or analysts to define policies, thresholds, or conditional behaviors through declarative logic. These rules are maintained outside the application codebase and can be updated independently.",
								"examples": ["Drools", "Red Hat Decision Manager", "FICO Blaze Advisor"],
								"benefits": [
									"Faster time-to-change",
									"Separation of concerns",
									"Better auditability",
									"Business-user ownership of logic"
								]
							}
						]
					}
				]
			},
			{
				"id": "application-interaction",
				"name": "Application Interaction",
				"description": "Tools for user interaction, UI behavior, user session handling, and presentation orchestration",
				"subcategories": [
					{
						"id": "application-composition",
						"name": "Application Composition",
						"description": "Tools for building and composing applications from independent components",
						"capabilities": [
							{
								"id": "modular-frontend-frameworks",
								"name": "Modular Frontend Frameworks",
								"definition": "The ability to compose frontends from independently developed and deployable micro frontends or components.",
								"description": "Modular frontend frameworks allow teams to build applications that are split across multiple domains or teams, while maintaining a cohesive user experience. Each module can be deployed independently and upgraded without affecting the entire frontend.",
								"examples": ["Module Federation", "Single-SPA", "Bit"],
								"benefits": [
									"Improved scalability",
									"Team autonomy",
									"Simplified upgrades",
									"Better ownership boundaries"
								]
							},
							{
								"id": "plugin-frameworks",
								"name": "Plugin Frameworks",
								"definition": "The capability to dynamically extend an applicationâ€™s features through independently developed modules or plugins.",
								"description": "Plugin frameworks provide an interface through which third parties or internal developers can add new functionality without modifying the core codebase. Common in extensible apps such as editors, platforms, and developer tools.",
								"examples": ["VS Code Extensions", "Grafana Plugins", "Jenkins Plugins"],
								"benefits": [
									"Rapid extensibility",
									"Community contribution",
									"Ecosystem growth",
									"Reduced core complexity"
								]
							}
						]
					},
					{
						"id": "state-navigation",
						"name": "State & Navigation",
						"description": "Tools for managing and navigating application state",
						"capabilities": [
							{
								"id": "state-management",
								"name": "State Management",
								"definition": "The capability to manage, share, and persist application state across components, views, or services.",
								"description": "State management libraries provide centralized or scoped approaches to handle dynamic data such as user inputs, responses, or derived values. These are especially important in reactive and SPAs where data must flow between UI layers and services.",
								"examples": ["Redux", "Zustand", "RxJS", "Svelte Stores"],
								"benefits": [
									"Predictable behavior",
									"Debuggability",
									"Improved consistency",
									"Scalable frontend architecture"
								]
							}
						]
					},
					{
						"id": "form-content-systems",
						"name": "Form & Content Systems",
						"description": "Tools for building and managing forms and content systems",
						"capabilities": [
							{
								"id": "form-builders",
								"name": "Form Builders",
								"definition": "The capability to build and render dynamic, schema-driven forms at runtime.",
								"description": "Form builders enable developers or non-technical users to create and manage forms for data entry, validation, and submission. They support dynamic schemas, conditional logic, and integrations with backend APIs.",
								"examples": ["Form.io", "React Hook Form", "Superforms"],
								"benefits": [
									"Faster form creation",
									"Lower maintenance",
									"Reusability",
									"Better UX for complex input flows"
								]
							}
						]
					}
				]
			},
			{
				"id": "application-integration",
				"name": "Application Integration",
				"description": "Integration and collaboration tools for applications",
				"subcategories": [
					{
						"id": "messaging-patterns",
						"name": "Messaging Patterns",
						"description": "Tools for managing and orchestrating integration flows",
						"capabilities": [
							{
								"id": "message-routing",
								"name": "Message Routing",
								"definition": "The capability to direct messages to the correct processing component or service based on content or metadata.",
								"description": "This includes routing patterns such as Content-Based Router, Routing Slip, and Recipient List that guide message delivery dynamically based on message properties.",
								"examples": [
									"Apache Camel Routing Slip",
									"Spring Integration Routers",
									"MuleSoft Flow Routers"
								],
								"benefits": [
									"Improved decoupling",
									"Dynamic workflows",
									"Supports complex process flows"
								]
							},
							{
								"id": "message-transformation",
								"name": "Message Transformation",
								"definition": "The capability to modify or translate messages between formats, protocols, or structures as they move between systems.",
								"description": "This includes Content Enricher, Message Translator, and Canonical Data Model patterns used to harmonize data between services.",
								"examples": [
									"Apache Camel Transformer",
									"MuleSoft DataWeave",
									"Spring Integration Transformers"
								],
								"benefits": ["Interoperability", "Improved data consistency", "Reduced coupling"]
							},
							{
								"id": "messaging-endpoints",
								"name": "Messaging Endpoints",
								"definition": "The capability to connect services or applications via reusable, reliable messaging interfaces.",
								"description": "Includes endpoint patterns such as Inbound/Outbound Channel Adapters and Durable Subscribers that abstract communication protocols.",
								"examples": ["Kafka Consumers", "RabbitMQ Adapters", "JMS Queues"],
								"benefits": ["Reliable delivery", "Protocol abstraction", "Scalability"]
							}
						]
					},
					{
						"id": "integration-control-flow",
						"name": "Integration Control Flow",
						"description": "Tools for managing and orchestrating integration flows",
						"capabilities": [
							{
								"id": "aggregator-pattern",
								"name": "Aggregator Pattern",
								"definition": "The capability to combine messages or events from multiple sources into a single, unified message.",
								"description": "Used when multiple sources contribute partial data that needs to be correlated and emitted as a complete message.",
								"examples": [
									"Apache Camel Aggregator",
									"AWS Step Functions Join",
									"Spring Integration Aggregator"
								],
								"benefits": ["Data completeness", "Flexible timing", "Improved coordination"]
							},
							{
								"id": "splitter-pattern",
								"name": "Splitter Pattern",
								"definition": "The ability to break a single composite message into multiple sub-messages for parallel or independent processing.",
								"description": "Useful for breaking down payloads into units of work that can be handled individually, e.g., batch records.",
								"examples": [
									"Apache Camel Splitter",
									"Spring Integration Splitter",
									"AWS Lambda Event Processing"
								],
								"benefits": ["Parallelism", "Granular control", "Simplified processing logic"]
							},
							{
								"id": "resequencer-pattern",
								"name": "Resequencer Pattern",
								"definition": "The capability to reorder messages into a defined sequence before forwarding to the next component.",
								"description": "Critical for maintaining order when messages arrive out of sequence due to retries, async processing, or distributed systems.",
								"examples": [
									"Apache Camel Resequencer",
									"Custom Kafka Consumers",
									"JMS Ordering Filters"
								],
								"benefits": ["Data integrity", "Order preservation", "Predictable outcomes"]
							}
						]
					},
					{
						"id": "event-driven-architecture",
						"name": "Event-Driven Architecture",
						"description": "Tools for building event-driven architectures",
						"capabilities": [
							{
								"id": "event-stream-processing",
								"name": "Event Stream Processing",
								"definition": "The capability to continuously capture, analyze, and respond to events as they occur in real time.",
								"description": "Supports near real-time applications by enabling transformation, filtering, enrichment, and routing of event data as it flows through the system. Often implemented using stream processing engines.",
								"examples": ["Apache Flink", "Kafka Streams", "Kinesis Data Analytics"],
								"benefits": ["Real-time responsiveness", "Operational agility", "Reduced latency"]
							},
							{
								"id": "event-replay-retention",
								"name": "Event Replay & Retention",
								"definition": "The capability to store and replay historical events to reprocess, debug, or audit application behavior.",
								"description": "Useful in event-sourced systems or for restoring state by replaying a durable log of past events. Enables recovery, compliance, and new service onboarding with historical data.",
								"examples": ["Kafka Log Compaction", "EventStoreDB", "AWS EventBridge Archive"],
								"benefits": [
									"Improved auditability",
									"Failure recovery",
									"Support for late consumers"
								]
							},
							{
								"id": "event-contracts-schemas",
								"name": "Event Contracts & Schemas",
								"definition": "The capability to define, manage, and evolve structured formats and agreements for event payloads.",
								"description": "Supports safe evolution and interoperability in distributed systems by providing versioned schemas and validation. Encourages reuse and reduces tight coupling between publishers and consumers.",
								"examples": ["AsyncAPI", "Apache Avro", "Confluent Schema Registry"],
								"benefits": [
									"Loose coupling",
									"Backward/forward compatibility",
									"Improved developer coordination"
								]
							}
						]
					}
				]
			}
		]
	},
	{
		"id": "data",
		"name": "Data Architecture",
		"description": "Data assets, storage, and management across the organization",
		"icon": "Database",
		"color": "bg-purple-heart-50 border-purple-heart-200 hover:bg-purple-heart-100",
		"categories": [
			{
				"id": "analytics-warehousing",
				"name": "Analytics & Warehousing",
				"description": "Data warehousing and analytics capabilities",
				"capabilities": [
					{
						"id": "data-warehousing",
						"name": "Data Warehousing",
						"definition": "The capability to store and analyze large volumes of structured data in a centralized repository",
						"description": "Data Warehousing provides a centralized repository for storing historical and current data from multiple sources, optimized for analytical queries and reporting. It enables organizations to perform complex analytics, generate insights, and support decision-making with high-performance data access.",
						"examples": [
							"Enterprise data warehouse implementation",
							"Data mart creation for specific departments",
							"Historical data analysis and reporting",
							"OLAP cube development",
							"Data warehouse automation",
							"Multi-dimensional data modeling"
						],
						"benefits": [
							"Centralized data storage and access",
							"Optimized query performance",
							"Historical data analysis capabilities",
							"Consistent data definitions",
							"Enhanced business intelligence",
							"Scalable analytical workloads"
						],
						"keyComponents": [
							"ETL/ELT processes",
							"Data modeling and schemas",
							"Query optimization",
							"Data governance",
							"Performance monitoring"
						],
						"technologies": [
							"Snowflake",
							"Amazon Redshift",
							"Azure Synapse Analytics",
							"Google BigQuery",
							"Teradata"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Data warehousing methodologies, dimensional modeling guides, and ETL design patterns available"
							},
							"build": {
								"available": true,
								"description": "Data warehouse platforms (Snowflake, Redshift), ETL tools, and data modeling software available"
							},
							"operate": {
								"available": true,
								"description": "Managed data warehouse services, cloud analytics platforms, and data pipeline solutions available"
							},
							"score": 3
						}
					},
					{
						"id": "real-time-analytics",
						"name": "Real-time Analytics",
						"definition": "The capability to process and analyze data as it arrives for immediate insights and actions",
						"description": "Real-time Analytics enables organizations to process streaming data and generate insights with minimal latency. It supports real-time decision making, fraud detection, monitoring, and immediate response to business events as they occur.",
						"examples": [
							"Stream processing of IoT sensor data",
							"Real-time fraud detection",
							"Live dashboards and monitoring",
							"Event-driven alerts and notifications",
							"Real-time personalization",
							"Operational intelligence and metrics"
						],
						"benefits": [
							"Immediate insights and responses",
							"Faster decision-making cycles",
							"Improved operational efficiency",
							"Enhanced customer experience",
							"Proactive issue detection",
							"Real-time competitive advantage"
						],
						"keyComponents": [
							"Stream processing engines",
							"Event ingestion systems",
							"Real-time data stores",
							"Analytics and visualization",
							"Alert and notification systems"
						],
						"technologies": [
							"Apache Kafka",
							"Azure Stream Analytics",
							"Amazon Kinesis",
							"Apache Flink",
							"Confluent Platform"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Real-time analytics patterns, streaming architecture guides, and event processing frameworks available"
							},
							"build": {
								"available": true,
								"description": "Stream processing platforms (Apache Kafka, Kinesis), real-time analytics tools, and event-driven architectures available"
							},
							"operate": {
								"available": true,
								"description": "Managed streaming services, real-time analytics platforms, and event-driven cloud solutions available"
							},
							"score": 3
						}
					},
					{
						"id": "data-pipelines-etl",
						"name": "Data Pipelines & ETL",
						"definition": "The capability to extract, transform, and load data from various sources into target systems through automated pipelines",
						"description": "Data Pipelines & ETL provide systematic approaches for moving and transforming data between systems, enabling data integration, cleansing, and preparation for analytics. They support both batch and real-time processing while ensuring data quality and consistency.",
						"examples": [
							"Automated data extraction from multiple sources",
							"Data transformation and cleansing operations",
							"Incremental data loading strategies",
							"Real-time data streaming pipelines",
							"Data quality validation and monitoring",
							"Error handling and data recovery processes"
						],
						"benefits": [
							"Automated data integration",
							"Improved data quality and consistency",
							"Reduced manual data processing",
							"Scalable data movement",
							"Enhanced data reliability",
							"Faster data availability for analytics"
						],
						"keyComponents": [
							"Data extraction tools",
							"Transformation engines",
							"Loading mechanisms",
							"Pipeline orchestration",
							"Monitoring and alerting"
						],
						"technologies": ["Apache Airflow", "Talend", "Informatica", "dbt", "Apache NiFi"],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "ETL architecture patterns and data pipeline design templates"
							},
							"build": {
								"available": true,
								"description": "Apache Airflow DAG templates, dbt transformation libraries"
							},
							"operate": {
								"available": true,
								"description": "Managed pipeline platforms (AWS Glue, Azure Data Factory)"
							},
							"score": 3
						}
					},
					{
						"id": "data-streaming",
						"name": "Data Streaming",
						"definition": "The capability to process continuous flows of data in real-time as it is generated",
						"description": "Data Streaming enables organizations to capture, process, and analyze data as it flows through systems in real-time. It supports event-driven architectures, real-time analytics, and immediate response to business events while handling high-volume, high-velocity data streams.",
						"examples": [
							"Real-time event stream processing",
							"Continuous data ingestion from IoT devices",
							"Live data transformation and enrichment",
							"Stream-based analytics and aggregations",
							"Real-time fraud detection",
							"Event-driven microservices communication"
						],
						"benefits": [
							"Real-time data processing capabilities",
							"Immediate insights and responses",
							"Scalable high-volume data handling",
							"Enhanced event-driven architectures",
							"Reduced data latency",
							"Better customer experience through real-time features"
						],
						"keyComponents": [
							"Stream processing engines",
							"Message brokers",
							"Stream analytics",
							"Event sourcing",
							"Real-time monitoring"
						],
						"technologies": [
							"Apache Kafka",
							"Apache Pulsar",
							"Amazon Kinesis",
							"Apache Flink",
							"Confluent Platform"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Event streaming architecture patterns and Kafka deployment guides"
							},
							"build": {
								"available": true,
								"description": "Stream processing libraries, Kafka client SDKs, and Flink templates"
							},
							"operate": {
								"available": true,
								"description": "Managed streaming platforms (Amazon Kinesis, Confluent Cloud)"
							},
							"score": 3
						}
					},
					{
						"id": "data-science-platforms",
						"name": "Data Science Platforms",
						"definition": "Capability to provide integrated environments for data science workflows, including data preparation, model development, training, and deployment with collaborative tools.",
						"description": "Data Science Platforms provide comprehensive environments for data scientists and analysts to perform end-to-end data science workflows. This capability includes data exploration, feature engineering, model development, training, validation, and deployment, along with collaboration tools and version control for data science projects.",
						"examples": [
							"Jupyter notebook environments",
							"Machine learning model development",
							"Data visualization and exploration",
							"Feature engineering pipelines",
							"Model training and validation",
							"Automated model deployment"
						],
						"benefits": [
							"Integrated data science workflows",
							"Collaborative development environment",
							"Accelerated model development",
							"Reproducible research and experiments",
							"Streamlined model deployment",
							"Enhanced data exploration capabilities"
						],
						"keyComponents": [
							"Notebook environments",
							"ML model development tools",
							"Data visualization frameworks",
							"Version control systems",
							"Compute resource management"
						],
						"technologies": [
							"Jupyter Hub",
							"Databricks",
							"AWS SageMaker",
							"Google AI Platform",
							"Azure Machine Learning"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Data science methodology templates and ML architecture patterns"
							},
							"build": {
								"available": true,
								"description": "Jupyter notebook templates, ML pipeline libraries, and model frameworks"
							},
							"operate": {
								"available": true,
								"description": "Managed ML platforms (AWS SageMaker, Azure ML, Databricks)"
							},
							"score": 3
						}
					}
				]
			},
			{
				"id": "data-management",
				"name": "Data Management",
				"description": "Core data management and governance capabilities",
				"capabilities": [
					{
						"id": "master-data-management",
						"name": "Master Data Management",
						"definition": "The capability to create and maintain a unified, consistent view of critical business data across all systems",
						"description": "Master Data Management (MDM) provides a single, authoritative source of truth for critical business entities such as customers, products, suppliers, and employees. It ensures data consistency, quality, and governance across all enterprise systems and applications.",
						"examples": [
							"Customer master data consolidation",
							"Product information management",
							"Supplier and vendor data management",
							"Employee master data governance",
							"Reference data management",
							"Data quality monitoring and improvement"
						],
						"benefits": [
							"Improved data consistency across systems",
							"Single source of truth for business entities",
							"Enhanced data quality and accuracy",
							"Better regulatory compliance",
							"Reduced data integration complexity",
							"Improved business decision making"
						],
						"keyComponents": [
							"Data integration and consolidation",
							"Data quality management",
							"Data governance workflows",
							"Master data repositories",
							"Data stewardship processes"
						],
						"technologies": [
							"Informatica MDM",
							"IBM InfoSphere MDM",
							"Talend MDM",
							"Stibo Systems",
							"Oracle MDM"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Master data management frameworks, data governance models, and data quality strategies available"
							},
							"build": {
								"available": true,
								"description": "MDM platforms (Informatica, Talend), data quality tools, and integration software available"
							},
							"operate": {
								"available": true,
								"description": "Managed MDM services, cloud data quality platforms, and data stewardship solutions available"
							},
							"score": 3
						}
					},
					{
						"id": "data-governance",
						"name": "Data Governance",
						"definition": "The capability to establish and maintain frameworks for managing data availability, usability, integrity, and security",
						"description": "Data Governance provides policies, processes, and standards for managing data as a strategic asset. It ensures data quality, compliance with regulations, and proper data stewardship across the organization while enabling secure and efficient data access for authorized users.",
						"examples": [
							"Data quality standards and monitoring",
							"Data classification and cataloging",
							"Data access controls and permissions",
							"Data retention and archiving policies",
							"Data privacy and compliance management",
							"Data stewardship and ownership"
						],
						"benefits": [
							"Improved data quality and consistency",
							"Enhanced regulatory compliance",
							"Better risk management and control",
							"Increased data trustworthiness",
							"Efficient data discovery and access",
							"Reduced data-related operational costs"
						],
						"keyComponents": [
							"Data governance policies",
							"Data quality frameworks",
							"Data cataloging and metadata",
							"Access control mechanisms",
							"Compliance monitoring"
						],
						"technologies": [
							"Collibra",
							"Alation",
							"Apache Atlas",
							"Informatica Axon",
							"Microsoft Purview"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Data governance frameworks, data stewardship models, and compliance strategies available"
							},
							"build": {
								"available": true,
								"description": "Data governance platforms, cataloging tools, and data lineage software available"
							},
							"operate": {
								"available": true,
								"description": "Managed data governance services, automated compliance platforms, and data stewardship solutions available"
							},
							"score": 3
						}
					},
					{
						"id": "data-cataloging",
						"name": "Data Cataloging",
						"definition": "The capability to discover, document, and organize data assets to enable efficient data discovery and understanding",
						"description": "Data Cataloging provides comprehensive metadata management and data discovery capabilities, enabling organizations to understand what data they have, where it resides, and how it can be used. It supports data lineage tracking, impact analysis, and self-service data discovery.",
						"examples": [
							"Automated data asset discovery",
							"Metadata management and documentation",
							"Data lineage and impact analysis",
							"Search and discovery interfaces",
							"Data relationship mapping",
							"Business glossary and data dictionary"
						],
						"benefits": [
							"Improved data discoverability",
							"Enhanced data understanding",
							"Faster time to insights",
							"Better data governance",
							"Reduced data redundancy",
							"Increased data reusability"
						],
						"keyComponents": [
							"Metadata repository",
							"Data discovery engines",
							"Lineage tracking systems",
							"Search interfaces",
							"Collaboration tools"
						],
						"technologies": [
							"Apache Atlas",
							"Alation",
							"Collibra",
							"DataHub",
							"AWS Glue Data Catalog"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Data catalog architecture patterns and metadata management frameworks"
							},
							"build": {
								"available": true,
								"description": "Apache Atlas templates, DataHub deployment guides, and catalog SDKs"
							},
							"operate": {
								"available": true,
								"description": "Managed data catalog services (AWS Glue Catalog, Alation Cloud)"
							},
							"score": 3
						}
					},
					{
						"id": "data-quality-management",
						"name": "Data Quality Management",
						"definition": "The capability to monitor, measure, and improve the accuracy, completeness, and reliability of data across the organization",
						"description": "Data Quality Management provides systematic approaches to assess and enhance data quality through profiling, cleansing, validation, and monitoring. It ensures data meets business requirements and maintains high standards for accuracy, consistency, and usability.",
						"examples": [
							"Data profiling and quality assessment",
							"Automated data validation rules",
							"Data cleansing and standardization",
							"Duplicate detection and resolution",
							"Data quality scorecards and metrics",
							"Continuous data quality monitoring"
						],
						"benefits": [
							"Improved data accuracy and reliability",
							"Enhanced decision-making confidence",
							"Reduced operational errors",
							"Better compliance with regulations",
							"Increased data trustworthiness",
							"Lower data management costs"
						],
						"keyComponents": [
							"Data profiling tools",
							"Quality rule engines",
							"Data cleansing processes",
							"Quality monitoring systems",
							"Issue resolution workflows"
						],
						"technologies": [
							"Informatica Data Quality",
							"Talend Data Quality",
							"IBM InfoSphere QualityStage",
							"SAS Data Management",
							"Great Expectations"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Data quality frameworks and validation rule templates"
							},
							"build": {
								"available": true,
								"description": "Great Expectations libraries, data profiling tools, and quality SDKs"
							},
							"operate": {
								"available": true,
								"description": "Managed data quality platforms (Informatica Cloud, Talend Cloud)"
							},
							"score": 3
						}
					},
					{
						"id": "metadata-management",
						"name": "Metadata Management",
						"definition": "Capability to collect, store, organize, and govern metadata across data assets, enabling better data discovery, understanding, and governance.",
						"description": "Metadata Management provides systematic approaches to capture and manage metadata about data assets, including technical, business, and operational metadata. This capability enables data discovery, lineage tracking, impact analysis, and enhanced data governance through comprehensive metadata catalogs.",
						"examples": [
							"Data catalog and discovery platforms",
							"Data lineage tracking and visualization",
							"Business glossary management",
							"Schema and data dictionary maintenance",
							"Data asset documentation",
							"Automated metadata extraction"
						],
						"benefits": [
							"Enhanced data discovery and understanding",
							"Improved data governance and compliance",
							"Better impact analysis capabilities",
							"Increased data transparency",
							"Accelerated data onboarding",
							"Reduced data redundancy"
						],
						"keyComponents": [
							"Metadata repository",
							"Data catalog systems",
							"Lineage tracking tools",
							"Business glossary",
							"Automated metadata extraction"
						],
						"technologies": [
							"Apache Atlas",
							"Collibra",
							"Alation",
							"AWS Glue Data Catalog",
							"Microsoft Purview"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Metadata governance frameworks and lineage mapping patterns"
							},
							"build": {
								"available": true,
								"description": "Apache Atlas APIs, metadata extraction tools, and governance SDKs"
							},
							"operate": {
								"available": true,
								"description": "Managed metadata platforms (Microsoft Purview, Collibra Cloud)"
							},
							"score": 3
						}
					},
					{
						"id": "data-mesh",
						"name": "Data Mesh",
						"definition": "Capability to implement decentralized data architecture where domain teams own and manage their data as products, with federated governance and self-serve infrastructure.",
						"description": "Data Mesh represents a paradigm shift from centralized data management to a decentralized approach where domain teams treat data as products. This capability enables scalable data architecture through domain ownership, federated governance, and self-serve data infrastructure platforms.",
						"examples": [
							"Domain-oriented data product development",
							"Federated computational governance",
							"Self-serve data infrastructure platforms",
							"Data product discovery and catalog",
							"Cross-domain data sharing protocols",
							"Automated data product lifecycle management"
						],
						"benefits": [
							"Scalable decentralized data architecture",
							"Improved data ownership and accountability",
							"Faster data product development",
							"Enhanced cross-domain collaboration",
							"Reduced central bottlenecks",
							"Better data product innovation"
						],
						"keyComponents": [
							"Domain data products",
							"Federated governance framework",
							"Self-serve data platform",
							"Data product registry",
							"Cross-domain interoperability"
						],
						"technologies": ["Kubernetes", "Apache Kafka", "dbt", "Apache Airflow", "Starburst"],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Data mesh principles and domain-driven architecture patterns"
							},
							"build": {
								"available": true,
								"description": "Data product templates, federated governance frameworks, and platform SDKs"
							},
							"operate": {
								"available": true,
								"description": "Managed data platform services (Databricks, Snowflake) support data mesh patterns"
							},
							"score": 3
						}
					}
				]
			},
			{
				"id": "storage-recovery",
				"name": "Storage & Recovery",
				"description": "Data storage and recovery capabilities",
				"capabilities": [
					{
						"id": "data-lake",
						"name": "Data Lake",
						"definition": "The capability to store vast amounts of raw data in native format for future processing and analysis",
						"description": "Data Lake provides a centralized repository for storing structured, semi-structured, and unstructured data at scale. It enables organizations to store data first and define schemas later, supporting various analytics, machine learning, and data processing workloads cost-effectively.",
						"examples": [
							"Raw data ingestion from multiple sources",
							"Structured and unstructured data storage",
							"Data lake as a staging area for data warehouses",
							"Big data analytics and processing",
							"Machine learning model training data",
							"Archive and compliance data storage"
						],
						"benefits": [
							"Cost-effective storage of large data volumes",
							"Flexible schema-on-read approach",
							"Support for diverse data types",
							"Scalable storage infrastructure",
							"Enhanced big data analytics capabilities",
							"Foundation for machine learning initiatives"
						],
						"keyComponents": [
							"Scalable storage infrastructure",
							"Data ingestion pipelines",
							"Data cataloging and discovery",
							"Access control and security",
							"Data processing frameworks"
						],
						"technologies": [
							"Azure Data Lake Storage",
							"Amazon S3",
							"Hadoop HDFS",
							"Google Cloud Storage",
							"MinIO"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Data lake architecture patterns, big data strategies, and data ingestion frameworks available"
							},
							"build": {
								"available": true,
								"description": "Data lake platforms (Hadoop, Spark), object storage systems, and data processing frameworks available"
							},
							"operate": {
								"available": true,
								"description": "Managed data lake services, cloud storage platforms, and big data analytics solutions available"
							},
							"score": 3
						}
					},
					{
						"id": "backup-recovery",
						"name": "Backup & Recovery",
						"definition": "The capability to protect and restore critical business data in case of system failures, disasters, or data corruption",
						"description": "Backup & Recovery provides comprehensive data protection strategies including automated backups, disaster recovery planning, and rapid restoration capabilities. It ensures business continuity by protecting against data loss and enabling quick recovery from various failure scenarios.",
						"examples": [
							"Automated database backups",
							"Disaster recovery site implementation",
							"Point-in-time recovery capabilities",
							"Cross-region data replication",
							"Backup testing and validation",
							"Recovery time objective (RTO) optimization"
						],
						"benefits": [
							"Protection against data loss",
							"Ensured business continuity",
							"Rapid disaster recovery",
							"Compliance with retention policies",
							"Reduced downtime and costs",
							"Enhanced data resilience"
						],
						"keyComponents": [
							"Backup scheduling and automation",
							"Data replication systems",
							"Recovery procedures",
							"Backup validation and testing",
							"Disaster recovery planning"
						],
						"technologies": [
							"Veeam Backup & Replication",
							"Commvault",
							"Azure Backup",
							"AWS Backup",
							"Veritas NetBackup"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Backup and recovery strategies, disaster recovery planning, and business continuity frameworks available"
							},
							"build": {
								"available": true,
								"description": "Backup software, disaster recovery tools, and data replication systems available"
							},
							"operate": {
								"available": true,
								"description": "Managed backup services, cloud disaster recovery platforms, and business continuity solutions available"
							},
							"score": 3
						}
					},
					{
						"id": "time-series-databases",
						"name": "Time Series Databases",
						"definition": "Capability to efficiently store, query, and analyze time-stamped data with optimized performance for temporal queries, real-time analytics, and high-frequency data ingestion.",
						"description": "Time Series Databases provide specialized storage and query capabilities for time-stamped data, enabling efficient handling of metrics, sensor data, logs, and events. This capability supports high-throughput ingestion, automatic data compression, time-based partitioning, and advanced analytical functions for temporal data patterns.",
						"examples": [
							"IoT sensor data collection",
							"Application performance metrics",
							"Financial market data storage",
							"System monitoring and alerting",
							"Real-time analytics dashboards",
							"Predictive maintenance systems"
						],
						"benefits": [
							"Optimized temporal query performance",
							"Automatic data compression and retention",
							"High-throughput data ingestion",
							"Built-in time-based analytics",
							"Efficient storage of timestamped data",
							"Real-time monitoring capabilities"
						],
						"keyComponents": [
							"Time-based data partitioning",
							"Compression algorithms",
							"Query optimization engines",
							"Data retention policies",
							"Real-time ingestion pipelines"
						],
						"technologies": [
							"InfluxDB",
							"TimescaleDB",
							"Amazon Timestream",
							"Apache Druid",
							"Prometheus"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Reference architectures and patterns for time-series data modeling"
							},
							"build": {
								"available": true,
								"description": "Time-series libraries, InfluxDB/TimescaleDB deployment templates"
							},
							"operate": {
								"available": true,
								"description": "Managed time-series platforms (AWS Timestream, InfluxDB Cloud)"
							},
							"score": 3
						}
					},
					{
						"id": "graph-databases",
						"name": "Graph Databases",
						"definition": "Capability to store, query, and analyze highly connected data using graph structures with nodes, edges, and properties, optimized for traversal and relationship queries.",
						"description": "Graph Databases provide specialized storage and query capabilities for data with complex relationships and connections. This capability enables efficient traversal of relationships, pattern matching, and analysis of network structures, making it ideal for social networks, recommendation engines, fraud detection, and knowledge graphs.",
						"examples": [
							"Social network analysis",
							"Recommendation systems",
							"Fraud detection networks",
							"Knowledge graph construction",
							"Supply chain optimization",
							"Identity and access management"
						],
						"benefits": [
							"Efficient relationship traversal",
							"Intuitive data modeling for connected data",
							"High-performance graph queries",
							"Scalable relationship analysis",
							"Pattern detection and matching",
							"Real-time graph analytics"
						],
						"keyComponents": [
							"Graph data models",
							"Traversal algorithms",
							"Query optimization engines",
							"Index structures",
							"Graph analytics functions"
						],
						"technologies": [
							"Neo4j",
							"Amazon Neptune",
							"ArangoDB",
							"Azure Cosmos DB (Gremlin)",
							"TigerGraph"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Graph data modeling patterns and Neo4j/Neptune architecture guides"
							},
							"build": {
								"available": true,
								"description": "Graph database client libraries and traversal query templates"
							},
							"operate": {
								"available": true,
								"description": "Managed graph database services (Amazon Neptune, Neo4j AuraDB)"
							},
							"score": 3
						}
					},
					{
						"id": "vector-databases",
						"name": "Vector Databases",
						"definition": "Capability to store, index, and query high-dimensional vector embeddings for similarity search, enabling efficient semantic search and AI/ML applications.",
						"description": "Vector Databases provide specialized storage and query capabilities for high-dimensional vector data, typically generated from machine learning models. This capability supports efficient similarity search, semantic search, and vector-based analytics, making it essential for AI applications like recommendation systems, image search, and natural language processing.",
						"examples": [
							"Semantic search applications",
							"Image and video similarity search",
							"Recommendation engines",
							"Natural language processing",
							"Computer vision applications",
							"Retrieval-augmented generation (RAG)"
						],
						"benefits": [
							"High-performance similarity search",
							"Scalable vector storage and indexing",
							"Efficient nearest neighbor queries",
							"Support for multiple distance metrics",
							"Real-time vector analytics",
							"Integration with ML pipelines"
						],
						"keyComponents": [
							"Vector indexing algorithms",
							"Similarity search engines",
							"Distance metric calculations",
							"Vector compression techniques",
							"Query optimization systems"
						],
						"technologies": ["Pinecone", "Weaviate", "Milvus", "Chroma", "Qdrant"],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Vector embedding patterns and similarity search architecture guidance"
							},
							"build": {
								"available": true,
								"description": "Vector database client SDKs and embedding generation libraries"
							},
							"operate": {
								"available": true,
								"description": "Managed vector database platforms (Pinecone, Weaviate Cloud)"
							},
							"score": 3
						}
					}
				]
			}
		]
	},
	{
		"id": "infrastructure",
		"name": "Infrastructure Architecture",
		"description": "Physical and virtual infrastructure supporting business operations",
		"icon": "Server",
		"color": "bg-mariner-50 border-mariner-200 hover:bg-mariner-100",
		"categories": [
			{
				"id": "application",
				"name": "Application",
				"description": "Application development and operations infrastructure",
				"subcategories": [
					{
						"id": "application-development",
						"name": "Application Development",
						"description": "Tools and processes for developing applications",
						"capabilities": [
							{
								"id": "development-tools",
								"name": "Development Tools",
								"definition": "Integrated environments and platforms that provide comprehensive toolsets for writing, building, testing, and debugging software applications",
								"description": "Development tools form the foundation of software engineering by providing developers with integrated environments for code creation, compilation, testing, and deployment. These platforms streamline development workflows, enforce coding standards, and integrate with version control systems to ensure consistent, high-quality software delivery across development teams.",
								"examples": [
									"Visual Studio Code for cross-platform development",
									"IntelliJ IDEA for Java enterprise applications",
									"Jenkins for automated build pipelines",
									"GitLab CI/CD for integrated development workflows",
									"Docker for containerized development environments",
									"Kubernetes for local cluster development"
								],
								"benefits": [
									"Accelerated development cycles through automated workflows",
									"Improved code quality via integrated testing and analysis",
									"Enhanced developer productivity with intelligent code assistance",
									"Consistent build processes across development environments",
									"Reduced debugging time through advanced diagnostic tools",
									"Streamlined collaboration via integrated version control"
								],
								"keyComponents": [
									"Integrated development environments",
									"Code editors and debuggers",
									"Build automation systems",
									"Testing frameworks",
									"Version control integration"
								],
								"technologies": [
									"Visual Studio Code",
									"IntelliJ IDEA",
									"Jenkins",
									"GitLab CI/CD",
									"Docker"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Development workflow patterns, toolchain architecture guides, and CI/CD pipeline templates available"
									},
									"build": {
										"available": true,
										"description": "IDE platforms, build tools, testing frameworks, and development environment configurations available"
									},
									"operate": {
										"available": true,
										"description": "Managed development platforms, cloud-based IDEs, and DevOps-as-a-service solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "code-analysis",
								"name": "Code Analysis",
								"definition": "Automated tools and platforms that perform static and dynamic analysis of source code to identify quality issues, security vulnerabilities, and compliance violations",
								"description": "Code analysis tools provide comprehensive evaluation of software code through automated scanning, testing, and review processes. These platforms integrate into development workflows to detect bugs, security flaws, and policy violations early in the development lifecycle, ensuring higher software quality and reduced security risks before deployment.",
								"examples": [
									"SonarQube for comprehensive code quality analysis",
									"Checkmarx for static application security testing",
									"Veracode for dynamic security vulnerability scanning",
									"ESLint for JavaScript code quality enforcement",
									"Prisma Cloud for infrastructure-as-code security scanning",
									"CodeClimate for automated code review and quality metrics"
								],
								"benefits": [
									"Early detection of defects reducing downstream costs",
									"Improved code quality through automated standards enforcement",
									"Enhanced security posture via vulnerability identification",
									"Reduced technical debt through continuous quality monitoring",
									"Accelerated code reviews through automated analysis",
									"Improved compliance with coding standards and regulations"
								],
								"keyComponents": [
									"Static analysis engines",
									"Dynamic analysis tools",
									"Code quality metrics",
									"Security vulnerability scanning",
									"Policy compliance checking"
								],
								"technologies": [
									"SonarQube",
									"Checkmarx",
									"Veracode",
									"ESLint",
									"Prisma Cloud"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Code quality frameworks, security scanning methodologies, and static analysis patterns available"
									},
									"build": {
										"available": true,
										"description": "Static analysis tools, security scanners, and code quality platforms available"
									},
									"operate": {
										"available": true,
										"description": "Managed code analysis services, cloud-based security scanning, and automated quality gates available"
									},
									"score": 3
								}
							},
							{
								"id": "infrastructure-as-code",
								"name": "Infrastructure as Code",
								"definition": "Methodology and tools for managing and provisioning computing infrastructure through machine-readable definition files rather than manual configuration processes",
								"description": "Infrastructure as Code (IaC) enables organizations to define, deploy, and manage infrastructure resources using declarative or imperative code-based approaches. This practice brings software development principles like version control, testing, and automation to infrastructure management, ensuring consistent, repeatable, and scalable infrastructure deployments across environments.",
								"examples": [
									"Terraform for multi-cloud infrastructure provisioning",
									"AWS CloudFormation for AWS-native resource management",
									"Ansible for configuration management and orchestration",
									"Pulumi for cloud-native infrastructure using familiar programming languages",
									"Chef for automated server configuration and compliance",
									"Azure Resource Manager templates for Azure infrastructure deployment"
								],
								"benefits": [
									"Faster infrastructure provisioning through automated deployment",
									"Reduced configuration drift via consistent code-based definitions",
									"Enhanced version control and change tracking for infrastructure",
									"Improved consistency and reusability across environments",
									"Decreased manual errors through automated provisioning processes",
									"Simplified disaster recovery with reproducible infrastructure"
								],
								"keyComponents": [
									"Infrastructure definition languages",
									"State management systems",
									"Resource provisioning engines",
									"Configuration management tools",
									"Pipeline integration frameworks"
								],
								"technologies": [
									"Terraform",
									"Ansible",
									"Chef",
									"Pulumi",
									"AWS CloudFormation"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Infrastructure as Code methodologies, Terraform patterns, and cloud architecture templates available"
									},
									"build": {
										"available": true,
										"description": "IaC tools (Terraform, Ansible), configuration management platforms, and infrastructure automation frameworks available"
									},
									"operate": {
										"available": true,
										"description": "Managed infrastructure services, cloud provisioning platforms, and infrastructure automation solutions available"
									},
									"score": 3
								}
							}
						]
					},
					{
						"id": "application-support-operations",
						"name": "Application Support & Operations",
						"description": "Tools for deploying, monitoring, and operating applications",
						"capabilities": [
							{
								"id": "packaging-distribution",
								"name": "Packaging & Distribution",
								"definition": "Systems and processes for storing, versioning, and distributing software build artifacts, libraries, and packages across development and production environments",
								"description": "Packaging and distribution capabilities provide centralized management of software artifacts including compiled applications, libraries, container images, and dependencies. These systems ensure secure storage, version control, and efficient distribution of software components across development pipelines and deployment environments, supporting both development workflows and production releases.",
								"examples": [
									"JFrog Artifactory for enterprise artifact management",
									"Sonatype Nexus Repository for open source and proprietary packages",
									"Docker Registry for container image storage and distribution",
									"AWS Elastic Container Registry for cloud-native container management",
									"Azure Container Registry for Microsoft ecosystem integration",
									"GitHub Package Registry for integrated source code and package management"
								],
								"benefits": [
									"Improved build consistency through centralized artifact management",
									"Enhanced security via artifact scanning and access controls",
									"Faster deployment cycles through optimized distribution networks",
									"Simplified dependency management with version control",
									"Reduced storage costs through deduplication and compression",
									"Better compliance tracking with audit trails and provenance"
								],
								"keyComponents": [
									"Artifact repositories",
									"Package management systems",
									"Version control mechanisms",
									"Distribution networks",
									"Security scanning integration"
								],
								"technologies": [
									"JFrog Artifactory",
									"Nexus Repository",
									"Docker Registry",
									"AWS ECR",
									"Azure Container Registry"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Artifact management strategies, package distribution patterns, and repository architecture guides available"
									},
									"build": {
										"available": true,
										"description": "Artifact repositories, package managers, and container registries available"
									},
									"operate": {
										"available": true,
										"description": "Managed artifact services, cloud repositories, and automated distribution platforms available"
									},
									"score": 3
								}
							},
							{
								"id": "release-automation",
								"name": "Release Automation",
								"definition": "Automated systems and workflows that manage application deployment, release orchestration, and environment promotion without manual intervention",
								"description": "Release automation capabilities streamline the deployment process by automating application releases across multiple environments through defined pipelines and workflows. These systems support GitOps methodologies, blue-green deployments, and canary releases, ensuring consistent, reliable, and repeatable application deployments with built-in rollback capabilities and environment management.",
								"examples": [
									"ArgoCD for Kubernetes-native GitOps deployments",
									"Flux for automated cluster state management",
									"Spinnaker for multi-cloud continuous delivery",
									"Octopus Deploy for .NET and Windows application releases",
									"Jenkins X for cloud-native CI/CD pipelines",
									"GitHub Actions for integrated source-to-deployment workflows"
								],
								"benefits": [
									"Accelerated release cycles through automated deployment pipelines",
									"Reduced manual intervention and human error in deployments",
									"Enhanced deployment reliability with consistent automated processes",
									"Simplified rollback procedures with version control integration",
									"Improved visibility into deployment status and environment health",
									"Better compliance through automated deployment approvals and auditing"
								],
								"keyComponents": [
									"Deployment pipelines",
									"Environment orchestration",
									"Release management workflows",
									"Rollback mechanisms",
									"Configuration management"
								],
								"technologies": [
									"ArgoCD",
									"Flux",
									"Spinnaker",
									"Octopus Deploy",
									"Jenkins X"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Deployment automation patterns, GitOps methodologies, and release management frameworks available"
									},
									"build": {
										"available": true,
										"description": "Deployment tools, CI/CD platforms, and release automation systems available"
									},
									"operate": {
										"available": true,
										"description": "Managed deployment services, cloud CI/CD platforms, and automated release solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "monitoring-observability",
								"name": "Monitoring & Observability",
								"definition": "Comprehensive platforms and practices for collecting, analyzing, and visualizing system telemetry data including metrics, logs, and traces to ensure application and infrastructure health",
								"description": "Monitoring and observability capabilities provide deep insights into system behavior through comprehensive telemetry collection, real-time alerting, and advanced analytics. These platforms enable proactive issue detection, rapid incident response, and performance optimization by correlating data from applications, infrastructure, and user experiences to maintain high system reliability and performance.",
								"examples": [
									"Grafana for unified dashboard visualization and alerting",
									"Prometheus for time-series metrics collection and monitoring",
									"New Relic for full-stack application performance monitoring",
									"Dynatrace for AI-powered observability and root cause analysis",
									"PagerDuty for incident management and escalation workflows",
									"Jaeger for distributed tracing and microservices observability"
								],
								"benefits": [
									"Faster incident detection and response through real-time alerting",
									"Increased system reliability via proactive monitoring and analysis",
									"Enhanced root cause analysis with correlated telemetry data",
									"Improved operational transparency across development and operations teams",
									"Reduced mean time to resolution for system issues",
									"Better capacity planning through performance trends and analytics"
								],
								"keyComponents": [
									"Metrics collection systems",
									"Log aggregation platforms",
									"Distributed tracing tools",
									"Alerting and notification systems",
									"Visualization and dashboarding"
								],
								"technologies": [
									"Grafana",
									"Prometheus",
									"New Relic",
									"Dynatrace",
									"PagerDuty"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Observability architecture patterns, monitoring strategies, and SRE frameworks available"
									},
									"build": {
										"available": true,
										"description": "Monitoring tools, observability platforms, and alerting systems available"
									},
									"operate": {
										"available": true,
										"description": "Managed monitoring services, cloud observability platforms, and SRE-as-a-service solutions available"
									},
									"score": 3
								}
							}
						]
					}
				]
			},
			{
				"id": "platforms",
				"name": "Platforms",
				"description": "Foundational platforms and services",
				"subcategories": [
					{
						"id": "ai-ml-automation",
						"name": "AI, ML & Automation",
						"description": "Artificial intelligence and machine learning platforms",
						"capabilities": [
							{
								"id": "ai-ml-automation",
								"name": "AI, ML & Automation",
								"definition": "Technologies that enable intelligent decision-making and process automation",
								"description": "This capability supports automation of tasks and insights generation using artificial intelligence and machine learning technologies.",
								"examples": ["Azure OpenAI", "SageMaker", "DataRobot", "ChatGPT", "AutoML"],
								"benefits": [
									"Faster insight generation",
									"Improved productivity",
									"Automated repetitive tasks",
									"Enhanced user experience"
								],
								"keyComponents": [
									"Machine learning platforms",
									"AI model deployment systems",
									"Automated workflow engines",
									"Natural language processing tools",
									"Intelligent analytics frameworks"
								],
								"technologies": [
									"Azure OpenAI",
									"AWS SageMaker",
									"DataRobot",
									"Google AutoML",
									"Hugging Face"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "AI/ML strategy frameworks, automation patterns, and intelligent system architectures available"
									},
									"build": {
										"available": true,
										"description": "ML platforms, AI development frameworks, and automation tools available"
									},
									"operate": {
										"available": true,
										"description": "Managed AI services, cloud ML platforms, and automated intelligence solutions available"
									},
									"score": 3
								}
							}
						]
					},
					{
						"id": "database",
						"name": "Database",
						"description": "Database platforms and services",
						"capabilities": [
							{
								"id": "document-store",
								"name": "Document Store",
								"definition": "Non-relational databases optimized for semi-structured document storage",
								"description": "Document stores are designed to store, retrieve, and manage data in document-oriented formats such as JSON or BSON, enabling flexibility in schema and performance for content-heavy applications.",
								"examples": ["MongoDB", "CouchDB", "RavenDB"],
								"benefits": [
									"Schema flexibility",
									"High scalability",
									"Optimized for content-rich apps",
									"Efficient query of nested data"
								],
								"keyComponents": [
									"Document storage engines",
									"Query processing systems",
									"Index management tools",
									"Replication mechanisms",
									"Sharding and distribution"
								],
								"technologies": [
									"MongoDB",
									"CouchDB",
									"Amazon DocumentDB",
									"RavenDB",
									"Azure Cosmos DB"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Document database patterns, schema design guides, and NoSQL architecture frameworks available"
									},
									"build": {
										"available": true,
										"description": "Document databases, NoSQL platforms, and document management systems available"
									},
									"operate": {
										"available": true,
										"description": "Managed document services, cloud NoSQL platforms, and document database solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "oltp-capture",
								"name": "OLTP & Capture",
								"definition": "Databases and systems optimized for online transaction processing",
								"description": "OLTP systems support high-throughput transactional workloads with rapid reads and writes, ensuring data integrity and fast response times for business-critical applications.",
								"examples": ["Microsoft SQL Server", "IBM DB2", "Sybase ASE", "Oracle Database"],
								"benefits": [
									"High availability",
									"Transactional consistency",
									"Low-latency performance",
									"Supports core business operations"
								],
								"keyComponents": [
									"Transaction processing engines",
									"Concurrency control systems",
									"Data integrity mechanisms",
									"Performance optimization tools",
									"Backup and recovery systems"
								],
								"technologies": [
									"Microsoft SQL Server",
									"Oracle Database",
									"IBM DB2",
									"PostgreSQL",
									"MySQL"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "OLTP architecture patterns, transaction design guides, and database optimization strategies available"
									},
									"build": {
										"available": true,
										"description": "OLTP databases, transaction processing systems, and database management platforms available"
									},
									"operate": {
										"available": true,
										"description": "Managed database services, cloud OLTP platforms, and transaction processing solutions available"
									},
									"score": 3
								}
							}
						]
					},
					{
						"id": "data-platform",
						"name": "Data Platform",
						"description": "Data processing and analytics platforms",
						"capabilities": [
							{
								"id": "data-caching",
								"name": "Data Caching",
								"definition": "High-performance, in-memory storage systems that temporarily store frequently accessed data to dramatically reduce access latency and improve application performance",
								"description": "Data caching capabilities provide ultra-fast data access through in-memory storage systems that cache frequently requested information closer to applications and users. These systems reduce database load, improve response times, and enhance user experience by storing data in high-speed memory with intelligent eviction policies and distributed caching strategies for scalable performance optimization.",
								"examples": [
									"Redis for high-performance in-memory data structures and caching",
									"Memcached for distributed memory object caching systems", 
									"Apache Ignite for in-memory computing and distributed caching",
									"Hazelcast for distributed in-memory data grids and caching",
									"Amazon ElastiCache for managed cloud-based caching services",
									"Caffeine for high-performance Java application caching libraries"
								],
								"benefits": [
									"Dramatically faster response times through in-memory data access",
									"Significantly reduced backend database load and query volume",
									"Enhanced user experience via improved application responsiveness",
									"Better scalability through distributed caching and load distribution",
									"Lower infrastructure costs by reducing database server requirements",
									"Improved application resilience with cache-first data access patterns"
								],
								"keyComponents": [
									"In-memory storage engines",
									"Cache eviction policies",
									"Data serialization systems",
									"Clustering and replication",
									"Performance monitoring tools"
								],
								"technologies": [
									"Redis",
									"Memcached",
									"Apache Ignite",
									"Hazelcast",
									"Amazon ElastiCache"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Caching strategies, data access patterns, and cache architecture designs available"
									},
									"build": {
										"available": true,
										"description": "Caching platforms, in-memory databases, and cache management tools available"
									},
									"operate": {
										"available": true,
										"description": "Managed cache services, cloud caching platforms, and cache-as-a-service solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "data-integration",
								"name": "Data Integration",
								"definition": "The ability to combine data from different sources, formats, or systems into a unified view or pipeline",
								"description": "Data Integration capabilities allow systems to connect, transform, and synchronize information across a wide variety of platforms. This includes both batch ETL and real-time data flows, and supports use cases ranging from business intelligence to cross-platform orchestration.",
								"examples": [
									"Talend",
									"Informatica",
									"Apache Nifi",
									"Fivetran",
									"dbt",
									"Kafka Connect"
								],
								"benefits": [
									"Reduced data silos",
									"Improved data quality",
									"Centralized data processing",
									"Enhanced analytics capabilities"
								],
								"keyComponents": [
									"Data connectors and adapters",
									"Transformation engines",
									"Data mapping tools",
									"Pipeline orchestration systems",
									"Data quality monitoring"
								],
								"technologies": [
									"Talend",
									"Informatica",
									"Apache NiFi",
									"Fivetran",
									"dbt"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Data integration patterns, ETL architecture guides, and data pipeline frameworks available"
									},
									"build": {
										"available": true,
										"description": "Integration platforms, ETL tools, and data pipeline development systems available"
									},
									"operate": {
										"available": true,
										"description": "Managed integration services, cloud data pipeline platforms, and integration-as-a-service solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "reactive-queries",
								"name": "Reactive Queries",
								"definition": "Automatically re-running queries or data-fetching logic in response to changes in application or external state",
								"description": "Reactive query systems allow applications to respond in real-time to changes in inputs, filters, cache state, or user interactions. This is increasingly used in frontend and fullstack development for performance-sensitive and UX-optimized applications.",
								"examples": [
									"TanStack Query (React Query, Svelte Query)",
									"Apollo Client",
									"RxJS with Observable Queries"
								],
								"benefits": [
									"Automatic cache invalidation",
									"Improved user experience",
									"Simplified async data loading",
									"Less boilerplate code"
								],
								"keyComponents": [
									"Query state management",
									"Cache invalidation systems",
									"Reactive data pipelines",
									"Real-time synchronization",
									"Background data fetching"
								],
								"technologies": [
									"TanStack Query",
									"Apollo Client",
									"RxJS",
									"SWR",
									"Relay"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Reactive programming patterns, query optimization strategies, and real-time data architecture guides available"
									},
									"build": {
										"available": true,
										"description": "Reactive query libraries, state management frameworks, and real-time data synchronization tools available"
									},
									"operate": {
										"available": true,
										"description": "Managed real-time services, cloud data synchronization platforms, and reactive data solutions available"
									},
									"score": 3
								}
							}
						]
					},
					{
						"id": "business-process-management",
						"name": "Business Process Management",
						"description": "Business process automation and management",
						"capabilities": [
							{
								"id": "process-automation",
								"name": "Process Automation",
								"definition": "Digital platforms and tools that automate business workflows, repetitive tasks, and decision-making processes through robotic process automation and workflow orchestration",
								"description": "Process automation capabilities enable organizations to digitize and streamline business operations by automating routine tasks, complex workflows, and decision processes. These platforms support robotic process automation (RPA), workflow orchestration, and intelligent automation through visual designers, rule engines, and integration capabilities that reduce manual effort and operational costs.",
								"examples": [
									"Microsoft Power Automate for low-code workflow automation",
									"UiPath for robotic process automation and AI-powered automation",
									"Appian for low-code business process management and case management",
									"Automation Anywhere for enterprise-scale intelligent automation",
									"Blue Prism for secure, scalable robotic process automation",
									"Zapier for cloud-based app integration and workflow automation"
								],
								"benefits": [
									"Increased operational productivity through automated task execution",
									"Reduced manual effort and human error in repetitive processes",
									"Improved process accuracy and consistency across operations",
									"Enhanced scalability of business operations without proportional staff increases",
									"Faster process execution and reduced cycle times",
									"Better compliance and audit trails through automated governance"
								],
								"keyComponents": [
									"Workflow orchestration engines",
									"Business process modeling tools",
									"Robotic process automation (RPA)",
									"Task scheduling systems",
									"Integration and connectors"
								],
								"technologies": [
									"Microsoft Power Automate",
									"UiPath",
									"Appian",
									"Automation Anywhere",
									"Blue Prism"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Business process automation strategies, workflow design patterns, and RPA implementation guides available"
									},
									"build": {
										"available": true,
										"description": "Process automation platforms, RPA tools, and workflow management systems available"
									},
									"operate": {
										"available": true,
										"description": "Managed automation services, cloud-based RPA platforms, and process automation solutions available"
									},
									"score": 3
								}
							}
						]
					},
					{
						"id": "middleware",
						"name": "Middleware",
						"description": "Integration and communication middleware",
						"capabilities": [
							{
								"id": "messaging",
								"name": "Messaging",
								"definition": "Asynchronous communication infrastructure that enables distributed systems to exchange messages through brokers, queues, and topics without direct coupling",
								"description": "Messaging capabilities provide robust, scalable communication between distributed applications and services through message brokers, queues, and publish-subscribe patterns. These systems enable fault-tolerant, asynchronous processing while supporting event-driven architectures, microservices communication, and enterprise integration patterns that enhance system resilience and performance.",
								"examples": [
									"Apache Kafka for high-throughput event streaming and real-time data pipelines",
									"RabbitMQ for reliable message queuing and routing in microservices",
									"IBM MQ for enterprise-grade messaging and transaction processing",
									"Amazon SQS for cloud-native queue services and serverless integration",
									"Azure Service Bus for hybrid cloud messaging and integration",
									"Redis Pub/Sub for low-latency message broadcasting and caching"
								],
								"benefits": [
									"Enhanced system resilience through asynchronous message processing",
									"Improved scalability via decoupled service communication",
									"Increased fault tolerance with message persistence and retry mechanisms",
									"Better performance through non-blocking asynchronous workflows",
									"Simplified integration between heterogeneous systems and platforms",
									"Reduced system complexity through event-driven architecture patterns"
								],
								"keyComponents": [
									"Message brokers",
									"Queue management systems",
									"Routing and filtering engines",
									"Persistence mechanisms",
									"Monitoring and management tools"
								],
								"technologies": [
									"Apache Kafka",
									"RabbitMQ",
									"IBM MQ",
									"Amazon SQS",
									"Azure Service Bus"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Messaging architecture patterns, event-driven design guides, and messaging topology frameworks available"
									},
									"build": {
										"available": true,
										"description": "Message brokers, queue systems, and messaging middleware platforms available"
									},
									"operate": {
										"available": true,
										"description": "Managed messaging services, cloud message brokers, and messaging-as-a-service solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "web-hosting",
								"name": "Web Hosting",
								"definition": "Serving web applications and content over the internet",
								"description": "Web hosting platforms enable the deployment and operation of web-based applications, APIs, and static content to end users.",
								"examples": ["Apache HTTP Server", "Nginx", "Tomcat", "IIS"],
								"benefits": [
									"Reliable web service delivery",
									"Scalability",
									"Flexible deployment options",
									"Supports diverse workloads"
								],
								"keyComponents": [
									"Web server engines",
									"Load balancing systems",
									"SSL/TLS certificate management",
									"Content delivery networks",
									"Health monitoring tools"
								],
								"technologies": [
									"Apache HTTP Server",
									"Nginx",
									"Apache Tomcat",
									"Microsoft IIS",
									"Cloudflare"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Web hosting architectures, load balancing patterns, and content delivery strategies available"
									},
									"build": {
										"available": true,
										"description": "Web servers, application servers, and hosting platforms available"
									},
									"operate": {
										"available": true,
										"description": "Managed hosting services, cloud web platforms, and hosting-as-a-service solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "workload-scheduling",
								"name": "Workload Scheduling",
								"definition": "Scheduling and automation of batch jobs and background processes",
								"description": "Workload schedulers coordinate and manage the execution of jobs based on dependencies and timing rules, ensuring resource efficiency and job completion.",
								"examples": ["Autosys", "Control-M", "Apache Airflow", "Tivoli Workload Scheduler"],
								"benefits": [
									"Reliable batch processing",
									"Optimized resource usage",
									"Job dependency handling",
									"Reduced manual intervention"
								],
								"keyComponents": [
									"Job scheduling engines",
									"Dependency management systems",
									"Resource allocation tools",
									"Monitoring and alerting",
									"Job execution frameworks"
								],
								"technologies": [
									"Apache Airflow",
									"Control-M",
									"Autosys",
									"Kubernetes CronJobs",
									"IBM Workload Scheduler"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Workload scheduling patterns, batch processing strategies, and job orchestration frameworks available"
									},
									"build": {
										"available": true,
										"description": "Scheduling platforms, workflow orchestration tools, and batch processing systems available"
									},
									"operate": {
										"available": true,
										"description": "Managed scheduling services, cloud batch platforms, and workload automation solutions available"
									},
									"score": 3
								}
							}
						]
					}
				]
			},
			{
				"id": "storage",
				"name": "Storage",
				"description": "Storage systems and data persistence solutions",
				"subcategories": [
					{
						"id": "offline-storage",
						"name": "Offline Storage",
						"description": "Long-term storage and archival solutions",
						"capabilities": [
							{
								"id": "archive",
								"name": "Archive",
								"definition": "Long-term storage of data for compliance and retrieval",
								"description": "Archiving solutions preserve historical data that must be retained for legal, regulatory, or business purposes while ensuring secure and efficient retrieval.",
								"examples": ["IBM Content Manager OnDemand", "Archive360", "Commvault Archive"],
								"benefits": [
									"Regulatory compliance",
									"Long-term data retention",
									"Reduced storage costs",
									"Secure access to historical data"
								],
								"keyComponents": [
									"Long-term storage systems",
									"Data retention policies",
									"Search and retrieval engines",
									"Compliance monitoring tools",
									"Data integrity verification"
								],
								"technologies": [
									"IBM Content Manager OnDemand",
									"Archive360",
									"Commvault Archive",
									"AWS Glacier",
									"Azure Archive Storage"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Data archiving strategies, compliance frameworks, and retention policy templates available"
									},
									"build": {
										"available": true,
										"description": "Archive storage systems, compliance platforms, and data retention tools available"
									},
									"operate": {
										"available": true,
										"description": "Managed archive services, cloud archival platforms, and compliance-as-a-service solutions available"
									},
									"score": 3
								}
							},
							{
								"id": "backup",
								"name": "Backup",
								"definition": "Replication of data for recovery and protection",
								"description": "Backup systems safeguard critical data by creating copies stored separately to enable recovery in the event of failure, corruption, or disaster.",
								"examples": [
									"Veeam",
									"Cohesity",
									"Virtual Tape Libraries",
									"Immutable Backup Solutions"
								],
								"benefits": [
									"Business continuity",
									"Data protection",
									"Disaster recovery",
									"Operational resilience"
								],
								"keyComponents": [
									"Backup scheduling systems",
									"Data replication engines",
									"Recovery management tools",
									"Storage optimization",
									"Disaster recovery procedures"
								],
								"technologies": [
									"Veeam Backup & Replication",
									"Cohesity",
									"Commvault",
									"AWS Backup",
									"Azure Backup"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Backup strategies, disaster recovery planning, and business continuity frameworks available"
									},
									"build": {
										"available": true,
										"description": "Backup software, recovery tools, and data protection systems available"
									},
									"operate": {
										"available": true,
										"description": "Managed backup services, cloud backup platforms, and backup-as-a-service solutions available"
									},
									"score": 3
								}
							}
						]
					}
				]
			},
			{
				"id": "compute",
				"name": "Compute",
				"description": "Computing resources and platforms",
				"subcategories": [
					{
						"id": "logical-compute",
						"name": "Logical Compute",
						"description": "Serverless and function-based computing",
						"capabilities": [
							{
								"id": "logical-compute",
								"name": "Logical Compute",
								"definition": "Abstracted compute environments for running applications without managing servers",
								"description": "Logical compute refers to serverless or function-based compute environments that abstract infrastructure management, allowing developers to focus on application logic.",
								"examples": ["AWS Lambda", "Azure Functions", "Google Cloud Functions"],
								"benefits": [
									"Simplified operations",
									"Scalable compute on demand",
									"Cost efficiency",
									"Faster deployment cycles"
								],
								"keyComponents": [
									"Serverless runtime environments",
									"Function execution engines",
									"Event-driven triggers",
									"Auto-scaling mechanisms",
									"Resource allocation systems"
								],
								"technologies": [
									"AWS Lambda",
									"Azure Functions",
									"Google Cloud Functions",
									"Vercel Functions",
									"Netlify Functions"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Serverless architecture patterns, function design guides, and event-driven frameworks available"
									},
									"build": {
										"available": true,
										"description": "Serverless platforms, function runtime environments, and event processing systems available"
									},
									"operate": {
										"available": true,
										"description": "Managed serverless services, cloud function platforms, and serverless-as-a-service solutions available"
									},
									"score": 3
								}
							}
						]
					},
					{
						"id": "mainframe",
						"name": "Mainframe",
						"description": "Enterprise mainframe computing platforms",
						"capabilities": [
							{
								"id": "mainframe",
								"name": "Mainframe",
								"definition": "Large-scale computing platforms supporting critical enterprise applications",
								"description": "Mainframes run batch and transaction processing workloads with high reliability, scalability, and security. They are essential for industries like banking and insurance.",
								"examples": ["IBM z/OS", "CICS", "COBOL Batch Jobs"],
								"benefits": [
									"Extreme reliability",
									"High-throughput processing",
									"Secure data handling",
									"Support for legacy systems"
								],
								"keyComponents": [
									"High-availability hardware",
									"Transaction processing systems",
									"Batch processing engines",
									"Security and encryption",
									"Legacy application frameworks"
								],
								"technologies": [
									"IBM z/OS",
									"CICS",
									"IBM DB2 for z/OS",
									"COBOL",
									"JCL (Job Control Language)"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Mainframe architecture patterns, legacy modernization strategies, and transaction processing frameworks available"
									},
									"build": {
										"available": true,
										"description": "Mainframe platforms, transaction systems, and legacy development tools available"
									},
									"operate": {
										"available": true,
										"description": "Managed mainframe services, cloud mainframe platforms, and mainframe modernization solutions available"
									},
									"score": 3
								}
							}
						]
					},
					{
						"id": "servers",
						"name": "Servers",
						"description": "Physical and virtual server infrastructure",
						"capabilities": [
							{
								"id": "servers",
								"name": "Servers",
								"definition": "Physical and virtual machines hosting applications and services",
								"description": "Servers provide the compute resources required to run enterprise workloads. They may be physical machines, virtualized environments, or part of hybrid infrastructure.",
								"examples": [
									"Dell PowerEdge servers",
									"VMware vSphere",
									"Hyper-V",
									"Bare metal provisioning"
								],
								"benefits": [
									"Scalable compute infrastructure",
									"Support for diverse workloads",
									"Dedicated hosting environments",
									"High availability options"
								],
								"keyComponents": [
									"Physical server hardware",
									"Virtualization platforms",
									"Operating system management",
									"Resource allocation systems",
									"High availability clustering"
								],
								"technologies": [
									"Dell PowerEdge",
									"VMware vSphere",
									"Microsoft Hyper-V",
									"Red Hat KVM",
									"Citrix XenServer"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "Server architecture patterns, virtualization strategies, and compute infrastructure frameworks available"
									},
									"build": {
										"available": true,
										"description": "Server hardware, virtualization platforms, and compute management systems available"
									},
									"operate": {
										"available": true,
										"description": "Managed server services, cloud compute platforms, and infrastructure-as-a-service solutions available"
									},
									"score": 3
								}
							}
						]
					}
				]
			},
			{
				"id": "it-management",
				"name": "IT Management",
				"description": "IT service management and operations",
				"subcategories": [
					{
						"id": "it-management",
						"name": "IT Management",
						"description": "IT service management and operations",
						"capabilities": [
							{
								"id": "it-management",
								"name": "IT Management",
								"definition": "Tools and practices for managing IT assets, incidents, and services",
								"description": "IT management tools support ITSM practices such as service requests, change management, and incident resolution, enhancing operational efficiency and user support.",
								"examples": ["ServiceNow", "BMC Remedy", "Freshservice", "Ivanti"],
								"benefits": [
									"Improved IT operations",
									"Enhanced service quality",
									"Faster issue resolution",
									"Governed change processes"
								],
								"keyComponents": [
									"Service desk platforms",
									"Incident management systems",
									"Change management workflows",
									"Asset tracking tools",
									"Service level management"
								],
								"technologies": [
									"ServiceNow",
									"BMC Remedy",
									"Freshservice",
									"Ivanti",
									"ManageEngine ServiceDesk Plus"
								],
								"maturityLevels": {
									"plan": {
										"available": true,
										"description": "ITSM frameworks, service management processes, and operational governance guides available"
									},
									"build": {
										"available": true,
										"description": "IT service management platforms, help desk systems, and operational tools available"
									},
									"operate": {
										"available": true,
										"description": "Managed IT service platforms, cloud-based ITSM solutions, and IT operations services available"
									},
									"score": 3
								}
							}
						]
					}
				]
			}
		]
	},
	{
		"id": "security",
		"name": "Security Architecture",
		"description": "Security controls, policies, and technologies protecting the enterprise",
		"icon": "Shield",
		"color": "bg-golf-50 border-golf-200 hover:bg-golf-100",
		"categories": [
			{
				"id": "identity-access",
				"name": "Identity & Access",
				"description": "Identity and access management solutions",
				"capabilities": [
					{
						"id": "identity-access-management",
						"name": "Identity & Access Management",
						"definition": "The capability to manage digital identities and control access to systems, applications, and resources",
						"description": "Identity & Access Management (IAM) provides centralized authentication, authorization, and user lifecycle management. It ensures that the right users have appropriate access to the right resources at the right time, while maintaining security and compliance requirements.",
						"examples": [
							"Single sign-on (SSO) implementation",
							"Multi-factor authentication (MFA)",
							"Role-based access control (RBAC)",
							"User provisioning and deprovisioning",
							"Identity federation and trust",
							"Access governance and reviews"
						],
						"benefits": [
							"Enhanced security through centralized access control",
							"Improved user experience with SSO",
							"Reduced administrative overhead",
							"Better compliance and audit capabilities",
							"Streamlined user lifecycle management",
							"Reduced risk of unauthorized access"
						],
						"keyComponents": [
							"Identity providers and directories",
							"Authentication mechanisms",
							"Authorization policies",
							"User provisioning systems",
							"Access governance tools"
						],
						"technologies": [
							"Active Directory",
							"Okta",
							"Azure Active Directory",
							"AWS IAM",
							"Ping Identity"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Identity and access management frameworks, zero-trust architecture guides, and authentication patterns available"
							},
							"build": {
								"available": true,
								"description": "IAM platforms (Okta, Active Directory), identity providers, and access control systems available"
							},
							"operate": {
								"available": true,
								"description": "Managed identity services, cloud IAM platforms, and identity governance solutions available"
							},
							"score": 3
						}
					}
				]
			},
			{
				"id": "threat-protection",
				"name": "Threat Protection",
				"description": "Threat detection and prevention capabilities",
				"capabilities": [
					{
						"id": "endpoint-protection",
						"name": "Endpoint Protection",
						"definition": "The capability to secure end-user devices and endpoints from malware, threats, and unauthorized access",
						"description": "Endpoint Protection provides comprehensive security for laptops, desktops, mobile devices, and servers through real-time threat detection, prevention, and response capabilities. It protects against malware, ransomware, and advanced persistent threats while maintaining device performance and user productivity.",
						"examples": [
							"Antivirus and anti-malware protection",
							"Behavioral threat detection",
							"Ransomware prevention",
							"Device compliance monitoring",
							"Endpoint detection and response (EDR)",
							"Mobile device management (MDM)"
						],
						"benefits": [
							"Comprehensive malware protection",
							"Real-time threat detection and response",
							"Enhanced device security posture",
							"Reduced security incident impact",
							"Improved compliance with security policies",
							"Centralized endpoint management"
						],
						"keyComponents": [
							"Endpoint agents and sensors",
							"Threat intelligence feeds",
							"Behavioral analysis engines",
							"Centralized management console",
							"Incident response capabilities"
						],
						"technologies": [
							"CrowdStrike Falcon",
							"Microsoft Defender",
							"Symantec Endpoint Protection",
							"Carbon Black",
							"SentinelOne"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Endpoint security strategies, threat detection frameworks, and security architecture patterns available"
							},
							"build": {
								"available": true,
								"description": "Endpoint protection platforms, EDR tools, and security monitoring systems available"
							},
							"operate": {
								"available": true,
								"description": "Managed endpoint security services, cloud security platforms, and SOC-as-a-service solutions available"
							},
							"score": 3
						}
					},
					{
						"id": "network-security",
						"name": "Network Security",
						"definition": "The capability to protect network infrastructure and communications from unauthorized access and threats",
						"description": "Network Security provides comprehensive protection for network infrastructure through firewalls, intrusion detection/prevention systems, and secure communication channels. It controls network access, monitors traffic, and prevents unauthorized activities while maintaining network performance and availability.",
						"examples": [
							"Firewall deployment and management",
							"Intrusion detection and prevention (IDS/IPS)",
							"Virtual private network (VPN) implementation",
							"Network access control (NAC)",
							"Network segmentation and microsegmentation",
							"Distributed denial-of-service (DDoS) protection"
						],
						"benefits": [
							"Enhanced network perimeter protection",
							"Controlled access to network resources",
							"Real-time threat detection and prevention",
							"Secure remote connectivity",
							"Compliance with security standards",
							"Reduced risk of network breaches"
						],
						"keyComponents": [
							"Firewalls and security appliances",
							"Intrusion detection systems",
							"Network monitoring tools",
							"VPN infrastructure",
							"Security policy enforcement"
						],
						"technologies": [
							"Cisco ASA",
							"Palo Alto Networks",
							"Fortinet FortiGate",
							"Check Point",
							"pfSense"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Network security architectures, segmentation strategies, and firewall design patterns available"
							},
							"build": {
								"available": true,
								"description": "Network security appliances, intrusion detection systems, and security monitoring tools available"
							},
							"operate": {
								"available": true,
								"description": "Managed network security services, cloud firewalls, and network security monitoring solutions available"
							},
							"score": 3
						}
					},
					{
						"id": "security-monitoring",
						"name": "Security Monitoring",
						"definition": "The capability to continuously monitor, analyze, and respond to security events across the enterprise",
						"description": "Security Monitoring provides 24/7 surveillance of security events through centralized logging, correlation, and analysis. It enables rapid threat detection, incident response, and compliance monitoring by aggregating security data from across the organization's infrastructure and applications.",
						"examples": [
							"Security information and event management (SIEM)",
							"Security operations center (SOC) operations",
							"Log aggregation and analysis",
							"Threat hunting and investigation",
							"Security incident response",
							"Compliance reporting and auditing"
						],
						"benefits": [
							"Real-time threat detection and alerting",
							"Centralized security event visibility",
							"Faster incident response times",
							"Enhanced compliance monitoring",
							"Proactive threat hunting capabilities",
							"Reduced security incident impact"
						],
						"keyComponents": [
							"Log collection and aggregation",
							"Event correlation engines",
							"Security analytics platforms",
							"Incident response workflows",
							"Compliance reporting tools"
						],
						"technologies": [
							"Splunk",
							"IBM QRadar",
							"Microsoft Sentinel",
							"Elastic Security",
							"LogRhythm"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Security monitoring frameworks, SIEM architecture patterns, and incident response strategies available"
							},
							"build": {
								"available": true,
								"description": "SIEM platforms (Splunk, QRadar), security analytics tools, and threat intelligence systems available"
							},
							"operate": {
								"available": true,
								"description": "Managed SIEM services, cloud security monitoring platforms, and SOC-as-a-service solutions available"
							},
							"score": 3
						}
					}
				]
			},
			{
				"id": "data-security",
				"name": "Data Security",
				"description": "Data protection and vulnerability management",
				"capabilities": [
					{
						"id": "data-protection",
						"name": "Data Protection",
						"definition": "The capability to safeguard sensitive data from unauthorized access, disclosure, and modification",
						"description": "Data Protection provides comprehensive security for sensitive information through encryption, access controls, data loss prevention, and privacy-preserving techniques. It ensures data confidentiality, integrity, and availability while maintaining compliance with regulatory requirements.",
						"examples": [
							"Data encryption at rest and in transit",
							"Data loss prevention (DLP) implementation",
							"Data masking and anonymization",
							"Access controls and permissions",
							"Data classification and labeling",
							"Privacy-preserving analytics"
						],
						"benefits": [
							"Enhanced data confidentiality",
							"Improved privacy compliance",
							"Reduced risk of data breaches",
							"Protection of intellectual property",
							"Regulatory compliance adherence",
							"Maintained customer trust"
						],
						"keyComponents": [
							"Encryption systems",
							"Key management infrastructure",
							"DLP policies and enforcement",
							"Access control mechanisms",
							"Data classification tools"
						],
						"technologies": [
							"Symantec DLP",
							"Microsoft Information Protection",
							"Forcepoint DLP",
							"Vera Suite",
							"Varonis"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Data protection strategies, encryption frameworks, and privacy compliance guides available"
							},
							"build": {
								"available": true,
								"description": "Data loss prevention tools, encryption software, and data classification systems available"
							},
							"operate": {
								"available": true,
								"description": "Managed data protection services, cloud encryption platforms, and data security monitoring solutions available"
							},
							"score": 3
						}
					},
					{
						"id": "vulnerability-management",
						"name": "Vulnerability Management",
						"definition": "The capability to systematically identify, assess, and remediate security vulnerabilities across the organization",
						"description": "Vulnerability Management provides a comprehensive approach to discovering, analyzing, and addressing security weaknesses in systems, applications, and infrastructure. It enables proactive security posture management through continuous scanning, risk assessment, and coordinated remediation efforts.",
						"examples": [
							"Vulnerability scanning and assessment",
							"Security patch management",
							"Penetration testing and validation",
							"Risk prioritization and scoring",
							"Remediation tracking and reporting",
							"Compliance vulnerability assessments"
						],
						"benefits": [
							"Proactive security risk reduction",
							"Improved security posture",
							"Enhanced compliance with standards",
							"Reduced attack surface",
							"Faster vulnerability remediation",
							"Better security investment decisions"
						],
						"keyComponents": [
							"Vulnerability scanners",
							"Risk assessment frameworks",
							"Patch management systems",
							"Remediation workflows",
							"Compliance reporting tools"
						],
						"technologies": [
							"Nessus",
							"Qualys VMDR",
							"Rapid7 InsightVM",
							"Greenbone",
							"Microsoft Defender for Endpoint"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Vulnerability management frameworks, risk assessment methodologies, and patch management strategies available"
							},
							"build": {
								"available": true,
								"description": "Vulnerability scanners, patch management tools, and security assessment platforms available"
							},
							"operate": {
								"available": true,
								"description": "Managed vulnerability services, cloud security scanning platforms, and automated patch management solutions available"
							},
							"score": 3
						}
					}
				]
			},
			{
				"id": "application-security",
				"name": "Application Security",
				"description": "Security controls for applications and development processes",
				"capabilities": [
					{
						"id": "secure-development",
						"name": "Secure Development",
						"definition": "The capability to integrate security controls and practices throughout the software development lifecycle",
						"description": "Secure Development embeds security considerations into every phase of software development, from design through deployment. It includes secure coding practices, security testing, dependency management, and continuous security validation to prevent vulnerabilities from entering production systems.",
						"examples": [
							"Secure coding standards and guidelines",
							"Static application security testing (SAST)",
							"Dynamic application security testing (DAST)",
							"Dependency vulnerability scanning",
							"Security code reviews",
							"Threat modeling and risk assessment"
						],
						"benefits": [
							"Reduced security vulnerabilities in applications",
							"Lower cost of vulnerability remediation",
							"Improved secure coding practices",
							"Enhanced application security posture",
							"Better compliance with security standards",
							"Faster secure software delivery"
						],
						"keyComponents": [
							"Secure coding frameworks",
							"Security testing tools",
							"Code review processes",
							"Threat modeling methodologies",
							"Dependency management systems"
						],
						"technologies": [
							"SonarQube",
							"Checkmarx",
							"Veracode",
							"OWASP ZAP",
							"Snyk"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Secure development frameworks, OWASP guidelines, and security-by-design principles available"
							},
							"build": {
								"available": true,
								"description": "Security testing tools, secure coding platforms, and vulnerability scanners available"
							},
							"operate": {
								"available": true,
								"description": "Managed application security services, cloud security testing platforms, and DevSecOps solutions available"
							},
							"score": 3
						}
					},
					{
						"id": "cloud-security",
						"name": "Cloud Security",
						"definition": "The capability to secure cloud infrastructure, services, and data across multi-cloud and hybrid environments",
						"description": "Cloud Security provides comprehensive protection for cloud-based assets through identity management, configuration security, workload protection, and compliance monitoring. It ensures secure cloud adoption while maintaining visibility and control across distributed cloud environments.",
						"examples": [
							"Cloud security posture management (CSPM)",
							"Cloud workload protection platforms (CWPP)",
							"Container and Kubernetes security",
							"Serverless security monitoring",
							"Cloud access security brokers (CASB)",
							"Multi-cloud security orchestration"
						],
						"benefits": [
							"Enhanced cloud infrastructure security",
							"Improved cloud compliance monitoring",
							"Reduced cloud misconfigurations",
							"Better visibility across cloud environments",
							"Secure cloud workload protection",
							"Streamlined multi-cloud security management"
						],
						"keyComponents": [
							"Cloud security monitoring",
							"Configuration management",
							"Workload protection",
							"Identity and access controls",
							"Compliance frameworks"
						],
						"technologies": [
							"AWS Security Hub",
							"Azure Security Center",
							"Google Cloud Security Command Center",
							"Prisma Cloud",
							"Aqua Security"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Cloud security frameworks, AWS/Azure security guides, and cloud architecture patterns available"
							},
							"build": {
								"available": true,
								"description": "Cloud security tools, CSPM platforms, and container security solutions available"
							},
							"operate": {
								"available": true,
								"description": "Managed cloud security services, cloud security platforms, and multi-cloud security solutions available"
							},
							"score": 3
						}
					}
				]
			},
			{
				"id": "security-governance",
				"name": "Security Governance",
				"description": "Security governance, training, and incident response capabilities",
				"capabilities": [
					{
						"id": "security-training",
						"name": "Security Training & Awareness",
						"definition": "The capability to educate and train employees on security best practices, threats, and organizational security policies",
						"description": "Security Training & Awareness builds a security-conscious culture through comprehensive education programs, simulated attacks, and continuous awareness campaigns. It empowers employees to recognize and respond to security threats while following established security protocols.",
						"examples": [
							"Security awareness training programs",
							"Phishing simulation campaigns",
							"Security policy training",
							"Incident response training",
							"Role-based security education",
							"Security culture assessment"
						],
						"benefits": [
							"Improved employee security awareness",
							"Reduced human error security incidents",
							"Enhanced security culture",
							"Better incident response capabilities",
							"Increased security policy compliance",
							"Stronger human firewall"
						],
						"keyComponents": [
							"Training content management",
							"Simulation platforms",
							"Assessment tools",
							"Awareness campaigns",
							"Progress tracking systems"
						],
						"technologies": [
							"KnowBe4",
							"Proofpoint Security Awareness",
							"Mimecast Awareness Training",
							"SANS Securing the Human",
							"Terranova Security"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Security training frameworks, awareness program templates, and security culture guides available"
							},
							"build": {
								"available": true,
								"description": "Security training platforms, phishing simulation tools, and awareness campaign systems available"
							},
							"operate": {
								"available": true,
								"description": "Managed security training services, cloud-based awareness platforms, and training-as-a-service solutions available"
							},
							"score": 3
						}
					},
					{
						"id": "incident-response",
						"name": "Incident Response",
						"definition": "The capability to detect, respond to, and recover from security incidents in a coordinated and effective manner",
						"description": "Incident Response provides structured processes and tools for handling security incidents from detection through recovery. It ensures rapid containment, thorough investigation, and coordinated response to minimize impact while maintaining evidence and enabling lessons learned.",
						"examples": [
							"Incident response plan development",
							"Security incident detection and triage",
							"Incident containment and eradication",
							"Forensic investigation and analysis",
							"Crisis communication and coordination",
							"Post-incident review and improvement"
						],
						"benefits": [
							"Faster incident detection and response",
							"Reduced security incident impact",
							"Improved incident coordination",
							"Better forensic investigation capabilities",
							"Enhanced recovery procedures",
							"Continuous incident response improvement"
						],
						"keyComponents": [
							"Incident response procedures",
							"Communication protocols",
							"Forensic tools",
							"Coordination platforms",
							"Recovery frameworks"
						],
						"technologies": [
							"Splunk Phantom",
							"IBM Resilient",
							"Demisto (Cortex XSOAR)",
							"ServiceNow Security Incident Response",
							"TheHive"
						],
						"maturityLevels": {
							"plan": {
								"available": true,
								"description": "Incident response frameworks, NIST guidelines, and response playbook templates available"
							},
							"build": {
								"available": true,
								"description": "Incident response platforms, SOAR tools, and forensic investigation systems available"
							},
							"operate": {
								"available": true,
								"description": "Managed incident response services, 24/7 SOC services, and incident response-as-a-service solutions available"
							},
							"score": 3
						}
					}
				]
			}
		]
	}
]
